{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae3a63-60c9-40cc-a15b-3968bbddf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "import shutil\n",
    "\n",
    "def load_config(config_path=\"config.json\"):\n",
    "    \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config['output_directory'], config['source_workbook_filename'], config['empty_census_workbook_filename'], config['clean_census_workbook_filename'], config['delete_census_workbook_filename']\n",
    "    \n",
    "\n",
    "# Test loading configuration\n",
    "output_directory, source_workbook_filename, empty_census_workbook_filename, clean_census_workbook_filename, delete_census_workbook_filename = load_config()\n",
    "print(\"Configuration loaded successfully.\")\n",
    "\n",
    "# Import some lookups\n",
    "%store -r core_lookups student_lookups censusworkbook_lookups\n",
    "%store -r df_enrolments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e5b30-4609-4c19-9dd2-2c7af3a2ba18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load workbook\n",
    "\n",
    "# Combine the directory and filename\n",
    "workbook_path = os.path.join(output_directory, source_workbook_filename)\n",
    "\n",
    "# Load the workbook (but not yet any sheet into memory)\n",
    "xls = pd.ExcelFile(workbook_path)\n",
    "\n",
    "# See the available sheet names\n",
    "print(\"Sheets found:\", xls.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882eaf0b-eb2e-4a2d-98a5-422aaa1e8ad3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get data from raw rosters\n",
    "\n",
    "# Build the full path\n",
    "workbook_path = os.path.join(output_directory, source_workbook_filename)\n",
    "\n",
    "# Load the Excel file\n",
    "xls = pd.ExcelFile(workbook_path)\n",
    "\n",
    "# Specify target sheets\n",
    "target_sheets = ['Newton', 'Larul', 'Maidher', 'Calvin', 'Charlene']\n",
    "\n",
    "# Prepare a list to collect valid DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Track the expected columns\n",
    "expected_columns = None\n",
    "\n",
    "# Load each sheet and validate structure\n",
    "for sheet in target_sheets:\n",
    "    df = xls.parse(sheet)\n",
    "\n",
    "    # Initialize expected columns from the first sheet\n",
    "    if expected_columns is None:\n",
    "        expected_columns = list(df.columns)\n",
    "    else:\n",
    "        # Check if the columns match exactly\n",
    "        if list(df.columns) != expected_columns:\n",
    "            raise ValueError(f\"Sheet '{sheet}' does not match expected columns: {expected_columns}\")\n",
    "\n",
    "    # Add source sheet name\n",
    "    df['SourceSheet'] = sheet\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all validated DataFrames\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display basic preview\n",
    "display(combined_df.head(3))\n",
    "\n",
    "# Enhanced summary\n",
    "total_rows = combined_df.shape[0]\n",
    "column_list = list(combined_df.columns)\n",
    "\n",
    "print(f\"‚úÖ Successfully combined {len(dfs)} sheets into one DataFrame with {total_rows} total rows.\\n\")\n",
    "print(f\"üßæ Columns in combined_df ({len(column_list)}):\")\n",
    "print(column_list)\n",
    "\n",
    "# Summary per SchoolName (if present)\n",
    "if 'SchoolName' in combined_df.columns:\n",
    "    print(\"\\nüìä Row counts per SchoolName:\")\n",
    "    print(combined_df['SchoolName'].value_counts(dropna=False).sort_index())\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è 'SchoolName' column not found in the combined DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c38e0-1eb3-4a32-b72a-ba3eb96c2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all rows with NaN and junk\n",
    "# Identify columns to check (exclude '0')\n",
    "columns_to_check = ['SchoolName', 'SchoolYear', 'Grade', 'FirstName', 'MiddleInitial', 'LastName', 'Gender', 'Sex', 'BirthDate']\n",
    "\n",
    "# Create a mask where we treat empty strings and whitespace as NaN\n",
    "df_check = combined_df[columns_to_check].replace(r'^\\s*$', pd.NA, regex=True)\n",
    "\n",
    "# Flag junk rows: all columns (except '0') are NA or empty\n",
    "junk_rows_mask = df_check.isna().all(axis=1)\n",
    "\n",
    "# Summary before dropping\n",
    "print(f\"üóëÔ∏è Identified {junk_rows_mask.sum()} junk row(s) where only column '0' has content.\")\n",
    "\n",
    "# Drop them\n",
    "combined_df_cleaned = combined_df[~junk_rows_mask].copy()\n",
    "\n",
    "# Confirm cleanup\n",
    "print(f\"‚úÖ Cleaned DataFrame now has {combined_df_cleaned.shape[0]} rows (was {combined_df.shape[0]}).\")\n",
    "\n",
    "# Display rows where SchoolName is still missing\n",
    "nan_schoolname_rows = combined_df_cleaned[combined_df_cleaned['SchoolName'].isna()]\n",
    "\n",
    "# Summary and preview\n",
    "print(f\"‚ö†Ô∏è There are {nan_schoolname_rows.shape[0]} row(s) with missing 'SchoolName' still present.\")\n",
    "display(nan_schoolname_rows.head(10))\n",
    "\n",
    "# Subset of rows where SchoolName is NaN\n",
    "nan_schoolname_rows = combined_df_cleaned[combined_df_cleaned['SchoolName'].isna()]\n",
    "\n",
    "# Identify columns that are not entirely NaN in these rows\n",
    "non_empty_columns = nan_schoolname_rows.dropna(axis=1, how='all').columns\n",
    "\n",
    "# Display the subset with only those columns\n",
    "print(f\"üìå Showing {len(non_empty_columns)} columns with data in rows with missing 'SchoolName':\")\n",
    "display(nan_schoolname_rows[non_empty_columns].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d10f9-34bb-4dfc-826c-0f74fee12e4c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean schNo\n",
    "\n",
    "# Step 1: Extract valid school codes from lookup\n",
    "valid_school_codes = {entry['C'] for entry in core_lookups['schoolCodes']}\n",
    "\n",
    "# Step 2: Flag invalid school codes in the DataFrame\n",
    "combined_df_cleaned['Invalid_schNo'] = ~combined_df_cleaned['schNo'].isin(valid_school_codes)\n",
    "\n",
    "# Step 3: Get a list of unique invalid school codes\n",
    "invalid_schNo_values = combined_df_cleaned.loc[combined_df_cleaned['Invalid_schNo'], 'schNo'].unique().tolist()\n",
    "total_invalid_rows = combined_df_cleaned['Invalid_schNo'].sum()\n",
    "\n",
    "# Step 4: Summary\n",
    "print(f\"‚ö†Ô∏è Flagged {total_invalid_rows} row(s) with invalid school codes.\")\n",
    "print(f\"‚ö†Ô∏è Found {len(invalid_schNo_values)} unique invalid schNo value(s):\")\n",
    "print(invalid_schNo_values)\n",
    "\n",
    "# Step 5: Build a lookup dictionary from school name to school code\n",
    "name_to_code = {entry['N']: entry['C'] for entry in core_lookups['schoolCodes']}\n",
    "\n",
    "# Step 6: Try to correct schNo based on SchoolName, only for invalid rows\n",
    "def correct_schno(row):\n",
    "    if row['Invalid_schNo']:\n",
    "        school_name = row.get('SchoolName')\n",
    "        corrected_code = name_to_code.get(school_name)\n",
    "        if corrected_code:\n",
    "            return corrected_code\n",
    "    return row['schNo']  # Leave original if not corrected\n",
    "\n",
    "# Step 7: Apply correction\n",
    "combined_df_cleaned['Corrected_schNo'] = combined_df_cleaned.apply(correct_schno, axis=1)\n",
    "\n",
    "# Step 8: Optional: Re-check how many are still invalid after correction\n",
    "combined_df_cleaned['Corrected_Invalid_schNo'] = ~combined_df_cleaned['Corrected_schNo'].isin(name_to_code.values())\n",
    "\n",
    "# Step 9: Summary\n",
    "fixed_count = combined_df_cleaned['Invalid_schNo'].sum() - combined_df_cleaned['Corrected_Invalid_schNo'].sum()\n",
    "print(f\"‚úÖ Automatically corrected {fixed_count} invalid schNo values based on SchoolName match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd979c76-64fd-4520-8b29-8d6fd5b220eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean SchoolName\n",
    "\n",
    "# Step 1: Extract valid school names from lookup\n",
    "valid_school_names = {entry['N'].strip() for entry in core_lookups['schoolCodes'] if entry['N']}\n",
    "\n",
    "# Step 2: Flag invalid school names in the DataFrame\n",
    "combined_df_cleaned['Invalid_SchoolName'] = ~combined_df_cleaned['SchoolName'].isin(valid_school_names)\n",
    "\n",
    "# Step 3: Get a list of unique invalid school names\n",
    "invalid_schoolname_values = (\n",
    "    combined_df_cleaned.loc[combined_df_cleaned['Invalid_SchoolName'], 'SchoolName']\n",
    "    .dropna()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "total_invalid_names = combined_df_cleaned['Invalid_SchoolName'].sum()\n",
    "\n",
    "# Step 4: Summary\n",
    "print(f\"‚ö†Ô∏è Flagged {total_invalid_names} row(s) with invalid SchoolName.\")\n",
    "print(f\"‚ö†Ô∏è Found {len(invalid_schoolname_values)} unique invalid SchoolName value(s):\")\n",
    "print(invalid_schoolname_values)\n",
    "\n",
    "# Step 5: Try to correct SchoolName based on fuzzy match\n",
    "import difflib\n",
    "\n",
    "def correct_schoolname(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    best_match = difflib.get_close_matches(name.strip(), valid_school_names, n=1, cutoff=0.7)\n",
    "    return best_match[0] if best_match else name  # fallback to original if no good match\n",
    "\n",
    "# Step 6: Apply correction to only invalid names\n",
    "combined_df_cleaned['CleanedSchoolName'] = combined_df_cleaned.apply(\n",
    "    lambda row: correct_schoolname(row['SchoolName']) if row['Invalid_SchoolName'] else row['SchoolName'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 7: Optional: Re-check how many are still invalid\n",
    "combined_df_cleaned['Corrected_Invalid_SchoolName'] = ~combined_df_cleaned['CleanedSchoolName'].isin(valid_school_names)\n",
    "\n",
    "# Step 8: Summary\n",
    "fixed_names_count = combined_df_cleaned['Invalid_SchoolName'].sum() - combined_df_cleaned['Corrected_Invalid_SchoolName'].sum()\n",
    "print(f\"‚úÖ Automatically corrected {fixed_names_count} invalid SchoolName values based on fuzzy match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349dc511-aa8b-4ea8-bbd2-5a6dcd9b86b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean SchoolYear\n",
    "\n",
    "# Step 1: Define expected value\n",
    "expected_school_year = '2024-2025'\n",
    "\n",
    "# Step 2: Flag invalid or missing SchoolYear values\n",
    "invalid_school_year_mask = combined_df_cleaned['SchoolYear'] != expected_school_year\n",
    "\n",
    "# Step 3: Create a DataFrame with problematic rows\n",
    "invalid_school_year_df = combined_df_cleaned[invalid_school_year_mask].copy()\n",
    "\n",
    "# Step 4: Summary\n",
    "print(f\"üìÖ Total rows in combined_df_cleaned: {combined_df_cleaned.shape[0]}\")\n",
    "print(f\"‚ö†Ô∏è Found {invalid_school_year_df.shape[0]} row(s) with invalid or missing SchoolYear.\")\n",
    "\n",
    "# Step 5: Optional preview\n",
    "if not invalid_school_year_df.empty:\n",
    "    print(\"\\nüìã Top 10 rows with invalid SchoolYear:\")\n",
    "    display(invalid_school_year_df[['SchoolName', 'SchoolYear', 'SourceSheet']].head(10))\n",
    "    \n",
    "    print(\"\\nüìä Count of invalid SchoolYear rows per SourceSheet:\")\n",
    "    print(invalid_school_year_df['SourceSheet'].value_counts().sort_index())\n",
    "\n",
    "# Step 6: Replace any non-matching or missing values with the correct one\n",
    "# Define the final cleaned format for SchoolYear\n",
    "formatted_school_year = f\"SY{expected_school_year}\"\n",
    "combined_df_cleaned['SchoolYear'] = combined_df_cleaned['SchoolYear'].where(\n",
    "    combined_df_cleaned['SchoolYear'] == formatted_school_year,\n",
    "    formatted_school_year\n",
    ")\n",
    "\n",
    "# Confirm result\n",
    "unique_years = combined_df_cleaned['SchoolYear'].unique()\n",
    "print(f\"‚úÖ All SchoolYear values set to '{expected_school_year}'. Unique values now: {unique_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb949f-4d3c-469a-9cc4-fecbe1cd7a58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean Grade levels\n",
    "# Step 1: Extract valid grade names from the lookup\n",
    "valid_grades = {entry['N'] for entry in core_lookups['levels']}\n",
    "\n",
    "# Step 2: Strip whitespace and unify formatting\n",
    "combined_df_cleaned['Grade'] = combined_df_cleaned['Grade'].astype(str).str.strip()\n",
    "\n",
    "# Step 3: Flag invalid grade values\n",
    "combined_df_cleaned['Invalid_Grade'] = ~combined_df_cleaned['Grade'].isin(valid_grades)\n",
    "\n",
    "# Step 4: Subset invalid rows\n",
    "invalid_grades_df = combined_df_cleaned[combined_df_cleaned['Invalid_Grade']].copy()\n",
    "\n",
    "# Step 5: Summary\n",
    "print(f\"üè´ Total rows: {combined_df_cleaned.shape[0]}\")\n",
    "print(f\"‚ö†Ô∏è Found {invalid_grades_df.shape[0]} row(s) with invalid or missing Grade.\")\n",
    "\n",
    "if not invalid_grades_df.empty:\n",
    "    display(invalid_grades_df[['SchoolName', 'Grade', 'SourceSheet']].head(10))\n",
    "    print(\"\\nüìä Count of invalid Grade values:\")\n",
    "    print(invalid_grades_df['Grade'].value_counts(dropna=False))\n",
    "\n",
    "# Step 6: Apply corrections to known mistakes\n",
    "grade_corrections = {\n",
    "    'Kiner': 'Kinder',\n",
    "    'Prek': 'Pre-K',\n",
    "    'pre-k': 'Pre-K',\n",
    "    'Grade1': 'Grade 1',\n",
    "    'grade 1': 'Grade 1',\n",
    "    '1st grade': 'Grade 1',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Apply correction\n",
    "combined_df_cleaned['Grade'] = combined_df_cleaned['Grade'].replace(grade_corrections)\n",
    "\n",
    "# Re-flag invalids after applying corrections\n",
    "combined_df_cleaned['Invalid_Grade'] = ~combined_df_cleaned['Grade'].isin(valid_grades)\n",
    "\n",
    "# Re-check how many remain invalid\n",
    "remaining_invalids = combined_df_cleaned['Invalid_Grade'].sum()\n",
    "print(f\"\\nüîÑ After applying corrections, {remaining_invalids} row(s) still have invalid Grade values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346d57d-0fb1-4566-9b68-38aea71bb1b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean and Validate Gender\n",
    "# Step 1: Define valid values\n",
    "valid_genders = {'Male', 'Female'}\n",
    "\n",
    "# Step 2: Strip whitespace and standardize type\n",
    "combined_df_cleaned['Gender'] = combined_df_cleaned['Gender'].astype(str).str.strip()\n",
    "\n",
    "# Step 3: Flag invalid values\n",
    "combined_df_cleaned['Invalid_Gender'] = ~combined_df_cleaned['Gender'].isin(valid_genders)\n",
    "\n",
    "# Step 4: Show initial invalids\n",
    "initial_invalids = combined_df_cleaned[combined_df_cleaned['Invalid_Gender']]\n",
    "print(f\"‚ö†Ô∏è Found {initial_invalids.shape[0]} row(s) with invalid Gender before correction.\")\n",
    "\n",
    "# Step 5: Apply basic corrections\n",
    "gender_corrections = {\n",
    "    'M': 'Male',\n",
    "    'F': 'Female',\n",
    "    'm': 'Male',\n",
    "    'f': 'Female',\n",
    "    'male': 'Male',\n",
    "    'female': 'Female',\n",
    "    'Femaler': 'Female',\n",
    "    'Feamale': 'Female',\n",
    "    'MALE': 'Male',\n",
    "    'FEMALE': 'Female',\n",
    "    'John': 'Male',\n",
    "}\n",
    "\n",
    "combined_df_cleaned['Gender'] = combined_df_cleaned['Gender'].replace(gender_corrections)\n",
    "\n",
    "# Step 6: Re-flag invalids after correction\n",
    "combined_df_cleaned['Invalid_Gender'] = ~combined_df_cleaned['Gender'].isin(valid_genders)\n",
    "\n",
    "# Step 7: Final summary\n",
    "remaining_invalids = combined_df_cleaned['Invalid_Gender'].sum()\n",
    "print(f\"‚úÖ Gender correction complete. Remaining invalid rows: {remaining_invalids}\")\n",
    "\n",
    "# Optional: Preview a few of the remaining invalids\n",
    "if remaining_invalids:\n",
    "    display(combined_df_cleaned[combined_df_cleaned['Invalid_Gender']][['SchoolName', 'Gender', 'SourceSheet']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baffc7-b04e-4899-9a45-81d245f54e44",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# BirthDate Cleaning and Age Flagging (Using Existing Context)\n",
    "\n",
    "# Step 1: Extract census year info for 2025\n",
    "census_info = next(x for x in censusworkbook_lookups['censusYears'] if x['svyYear'] == 2025)\n",
    "census_date = pd.to_datetime(census_info['svyCensusDate'])\n",
    "ps_age = census_info['svyPSAge']  # Age for Grade 1\n",
    "\n",
    "# Step 2: Build expected age mapping from grade names\n",
    "grade_age_map = {\n",
    "    entry['N']: ps_age-1 + entry['YoEd']\n",
    "    for entry in core_lookups['levels']\n",
    "    if 'YoEd' in entry and pd.notna(entry['YoEd'])\n",
    "}\n",
    "\n",
    "# Summary: show census date, ps_age, and sample grade mappings\n",
    "print(f\"üìÖ Census Date: {census_date.date()}\")\n",
    "print(f\"üéØ Official Age for Grade 1: {ps_age}\")\n",
    "print(f\"üìò Grade-to-Expected-Age Mapping (first 5):\")\n",
    "for grade, age in list(grade_age_map.items())[:5]:\n",
    "    print(f\"   - {grade}: {age} years old\")\n",
    "\n",
    "# Step 3: Parse BirthDate to datetime safely\n",
    "combined_df_cleaned['SourceParsedBirthDate'] = pd.to_datetime(\n",
    "    combined_df_cleaned['BirthDate'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Step 4: Calculate actual age at census date\n",
    "combined_df_cleaned['SourceAgeAtCensus'] = combined_df_cleaned['SourceParsedBirthDate'].apply(\n",
    "    lambda dob: census_date.year - dob.year - ((census_date.month, census_date.day) < (dob.month, dob.day))\n",
    "    if pd.notna(dob) else None\n",
    ")\n",
    "\n",
    "# Step 5: Determine expected age based on Grade\n",
    "combined_df_cleaned['ExpectedAge'] = combined_df_cleaned['Grade'].map(grade_age_map)\n",
    "\n",
    "# Step 6: Flag rows with unrealistic age difference (¬±3+ years from expected)\n",
    "combined_df_cleaned['AgeFlagged'] = combined_df_cleaned.apply(\n",
    "    lambda row: (\n",
    "        pd.notna(row['ExpectedAge']) and\n",
    "        pd.notna(row['SourceAgeAtCensus']) and\n",
    "        abs(row['SourceAgeAtCensus'] - row['ExpectedAge']) >= 3\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 7: Summary\n",
    "flagged_rows = combined_df_cleaned[combined_df_cleaned['AgeFlagged']]\n",
    "print(f\"‚ö†Ô∏è Found {flagged_rows.shape[0]} student(s) with BirthDate far from expected for their Grade.\")\n",
    "\n",
    "# Optional preview\n",
    "display(flagged_rows[['SchoolName', 'FirstName', 'LastName', 'Grade', 'BirthDate', 'SourceParsedBirthDate', 'SourceAgeAtCensus', 'ExpectedAge', 'SourceSheet']].head(10))\n",
    "\n",
    "# Step 8: Compare reported Age column with calculated AgeAtCensus\n",
    "# Make sure both are numeric and not null\n",
    "combined_df_cleaned['SourceReportedAge'] = pd.to_numeric(combined_df_cleaned['Age'], errors='coerce')\n",
    "\n",
    "combined_df_cleaned['AgeMismatch'] = combined_df_cleaned.apply(\n",
    "    lambda row: (\n",
    "        pd.notna(row['SourceReportedAge']) and\n",
    "        pd.notna(row['SourceAgeAtCensus']) and\n",
    "        row['SourceReportedAge'] != row['SourceAgeAtCensus']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Summary of mismatches\n",
    "mismatched_age_df = combined_df_cleaned[combined_df_cleaned['AgeMismatch']]\n",
    "print(f\"üîç Found {mismatched_age_df.shape[0]} row(s) where reported Age (calculated in the RMI raw rosters) differs from calculated AgeAtCensus.\")\n",
    "\n",
    "# Optional preview of mismatches\n",
    "display(mismatched_age_df[['SchoolName', 'FirstName', 'LastName', 'Grade', 'BirthDate', 'SourceParsedBirthDate', 'SourceAgeAtCensus', 'ExpectedAge', 'SourceReportedAge', 'SourceSheet']].head(10))\n",
    "\n",
    "# Step 9 (revised): Fix unrealistic or missing birthdates into FixedParsedBirthDate\n",
    "def get_default_birthdate(row):\n",
    "    if pd.isna(row['ExpectedAge']):\n",
    "        return None\n",
    "    expected_birth_year = census_date.year - row['ExpectedAge']\n",
    "    return pd.Timestamp(f\"{expected_birth_year}-01-01\")\n",
    "\n",
    "# Create the new column (default to original value)\n",
    "combined_df_cleaned['FixedParsedBirthDate'] = combined_df_cleaned['SourceParsedBirthDate']\n",
    "\n",
    "# Identify rows to fix\n",
    "birthdate_fix_mask = combined_df_cleaned['AgeFlagged'] | combined_df_cleaned['SourceParsedBirthDate'].isna()\n",
    "\n",
    "# Apply fix only to those rows\n",
    "combined_df_cleaned.loc[birthdate_fix_mask, 'FixedParsedBirthDate'] = combined_df_cleaned.loc[\n",
    "    birthdate_fix_mask\n",
    "].apply(get_default_birthdate, axis=1)\n",
    "\n",
    "# Add 'DoB Estimate' flag where we applied a fix\n",
    "combined_df_cleaned['DoB Estimate'] = ''\n",
    "combined_df_cleaned.loc[birthdate_fix_mask, 'DoB Estimate'] = 'Yes'\n",
    "\n",
    "# Recalculate AgeAtCensus using the fixed birthdates, and store in a new column\n",
    "combined_df_cleaned['FixedAgeAtCensus'] = combined_df_cleaned['FixedParsedBirthDate'].apply(\n",
    "    lambda dob: census_date.year - dob.year - ((census_date.month, census_date.day) < (dob.month, dob.day))\n",
    "    if pd.notna(dob) else None\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"üõ†Ô∏è Fixed {birthdate_fix_mask.sum()} BirthDate value(s) using January 1 of expected birth year (based on grade).\")\n",
    "print(f\"‚ÑπÔ∏è AgeFlagged column retained to indicate which rows were originally flagged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a608c1-1ce8-4d64-a091-326983bd2d7c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function to categorize age groups based on provided age column\n",
    "def categorize_age_group(df, age_column):\n",
    "    return df.apply(\n",
    "        lambda row: (\n",
    "            'Official Age' if pd.notna(row[age_column]) and pd.notna(row['ExpectedAge']) and row[age_column] == row['ExpectedAge']\n",
    "            else 'Under Age' if pd.notna(row[age_column]) and pd.notna(row['ExpectedAge']) and row[age_column] < row['ExpectedAge']\n",
    "            else 'Over Age' if pd.notna(row[age_column]) and pd.notna(row['ExpectedAge']) and row[age_column] > row['ExpectedAge']\n",
    "            else 'Unknown'\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# --- Plot using SourceAgeAtCensus ---\n",
    "combined_df_cleaned['SourceAgeGroup'] = categorize_age_group(combined_df_cleaned, 'SourceAgeAtCensus')\n",
    "\n",
    "grade_group_source = combined_df_cleaned[\n",
    "    combined_df_cleaned['Grade'].notna() & combined_df_cleaned['SourceAgeGroup'].notna()\n",
    "].groupby(['Grade', 'SourceAgeGroup']).size().unstack(fill_value=0)\n",
    "\n",
    "grade_order = [entry['N'] for entry in core_lookups['levels'] if entry['N'] in grade_group_source.index]\n",
    "grade_group_source = grade_group_source.reindex(grade_order)\n",
    "\n",
    "# Reorder age group columns\n",
    "age_order = ['Under Age', 'Official Age', 'Over Age']\n",
    "grade_group_source = grade_group_source[[col for col in age_order if col in grade_group_source.columns]]\n",
    "\n",
    "# Plot 1: All age groups (Source)\n",
    "ax1 = grade_group_source.plot(\n",
    "    kind='bar', stacked=False, figsize=(12, 6),\n",
    "    title='(Before Cleanup) Enrollment by Grade and Age Group',\n",
    "    ylabel='Number of Students', xlabel='Grade',\n",
    "    rot=45, grid=True, legend=True\n",
    ")\n",
    "for container in ax1.containers:\n",
    "    ax1.bar_label(container, label_type='edge', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Only Under and Over Age (Source)\n",
    "source_filtered = grade_group_source.drop(columns='Official Age', errors='ignore')\n",
    "ax2 = source_filtered.plot(\n",
    "    kind='bar', stacked=False, figsize=(12, 6),\n",
    "    title='(Before Cleanup) Enrollment by Grade: Under & Over Age Only',\n",
    "    ylabel='Number of Students', xlabel='Grade',\n",
    "    rot=45, grid=True, legend=True\n",
    ")\n",
    "for container in ax2.containers:\n",
    "    ax2.bar_label(container, label_type='edge', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot using FixedAgeAtCensus ---\n",
    "combined_df_cleaned['AgeGroup'] = categorize_age_group(combined_df_cleaned, 'FixedAgeAtCensus')\n",
    "\n",
    "grade_group_fixed = combined_df_cleaned[\n",
    "    combined_df_cleaned['Grade'].notna() & combined_df_cleaned['AgeGroup'].notna()\n",
    "].groupby(['Grade', 'AgeGroup']).size().unstack(fill_value=0)\n",
    "\n",
    "grade_group_fixed = grade_group_fixed.reindex(grade_order)\n",
    "grade_group_fixed = grade_group_fixed[[col for col in age_order if col in grade_group_fixed.columns]]\n",
    "\n",
    "# Plot 3: All age groups (Fixed)\n",
    "ax3 = grade_group_fixed.plot(\n",
    "    kind='bar', stacked=False, figsize=(12, 6),\n",
    "    title='(After Cleanup) Enrollment by Grade and Age Group',\n",
    "    ylabel='Number of Students', xlabel='Grade',\n",
    "    rot=45, grid=True, legend=True\n",
    ")\n",
    "for container in ax3.containers:\n",
    "    ax3.bar_label(container, label_type='edge', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Only Under and Over Age (Fixed)\n",
    "fixed_filtered = grade_group_fixed.drop(columns='Official Age', errors='ignore')\n",
    "ax4 = fixed_filtered.plot(\n",
    "    kind='bar', stacked=False, figsize=(12, 6),\n",
    "    title='(After Cleanup) Enrollment by Grade: Under & Over Age Only',\n",
    "    ylabel='Number of Students', xlabel='Grade',\n",
    "    rot=45, grid=True, legend=True\n",
    ")\n",
    "for container in ax4.containers:\n",
    "    ax4.bar_label(container, label_type='edge', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e2132-5dd2-46a8-9797-167578488fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combined_df_cleaned.iloc[:, 11:20])  # columns 1‚Äì5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1dd0ec-2a29-45db-9933-2114864b9f44",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Work on the From column\n",
    "\n",
    "# Step 1: Define initial mapping (you can expand this later)\n",
    "from_mapping = {\n",
    "    'Transferred IN': 'Transferred In',\n",
    "    'Transfer': 'Transferred In',\n",
    "    'Transfer From': 'Transferred In',\n",
    "    'Transfer In': 'Transferred In',\n",
    "    'Transfer in': 'Transferred In',\n",
    "    'Transferred In': 'Transferred In',\n",
    "    'Transferred in': 'Transferred In',\n",
    "    'Transferred IN': 'Transferred In',\n",
    "    '0': 'Transferred In',\n",
    "    'Carlos': 'Transferred In',\n",
    "    'EKN': 'Transferred In',\n",
    "    'EPES': 'Transferred In',\n",
    "    'New': 'New Enrolment',\n",
    "    'new': 'New Enrolment',\n",
    "    'NEW': 'New Enrolment',    \n",
    "    'Repeat': 'Repeater',\n",
    "    'Repeater': 'Repeater',\n",
    "    'Retained': 'Repeater',\n",
    "    'Retain': 'Repeater',\n",
    "    'Retained': 'Repeater',\n",
    "    'Retuening': np.nan,\n",
    "    'Returing': np.nan,\n",
    "    'Return/Retained': np.nan,\n",
    "    'Returning': np.nan,\n",
    "    'returning': np.nan,\n",
    "    'Returning': np.nan,\n",
    "    'Reurning': np.nan,\n",
    "    'Re-Entry': 'Repeater',\n",
    "    'Returned': np.nan,\n",
    "    'Continuing': np.nan,\n",
    "    'Continung': np.nan,\n",
    "    'Contiuing': np.nan,\n",
    "    '': np.nan,\n",
    "    # Add more mappings as needed...\n",
    "}\n",
    "\n",
    "# Step 2: Standardize 'From' values using the mapping\n",
    "combined_df_cleaned['CleanedFrom'] = combined_df_cleaned['Return/Retained'].astype(str).map(from_mapping)\n",
    "\n",
    "# Step 3: Show unique raw values in 'Return/Retained' for further mapping\n",
    "unique_values = combined_df_cleaned['Return/Retained'].dropna().astype(str).unique()\n",
    "\n",
    "print(\"üìã Unique values in 'Return/Retained':\")\n",
    "for val in sorted(unique_values, key=lambda x: x.lower()):\n",
    "    print(f\" - '{val}'\")\n",
    "\n",
    "# Step 4: Define final accepted values\n",
    "valid_from_values = {'ECE', 'New Enrolment', 'Repeater', 'Transferred In'}\n",
    "\n",
    "# Step 5: Count how many rows have valid 'From' values\n",
    "from_counts_before = combined_df_cleaned['CleanedFrom'].value_counts(dropna=False)\n",
    "valid_rows_before = combined_df_cleaned['CleanedFrom'].isin(valid_from_values).sum()\n",
    "invalid_rows_before = combined_df_cleaned.shape[0] - valid_rows_before\n",
    "\n",
    "# Step 6: Infer those that came from ECE\n",
    "# Set ECE values based on an existing enrolment record of previous year.\n",
    "# In other words, if a matching student df_enrolments -> combined_df_cleaned\n",
    "# ('stuGiven' = 'FirstName' and 'stuFamilyName' = 'FamilyName' and\n",
    "# 'stuDoB' = 'FixedParsedBirthDate') has a record in ECE last year\n",
    "# ('stueClass' = 'GK' and 'stueYear' = '2023-2024'), then set CleanedFrom = 'ECE'\n",
    "\n",
    "# --- Valid vs Invalid before ECE inference ---\n",
    "valid_from_values = {'ECE', 'New Enrolment', 'Repeater', 'Transferred In'}\n",
    "# Count valid values including NaNs\n",
    "valid_mask = combined_df_cleaned['CleanedFrom'].isin(valid_from_values) | combined_df_cleaned['CleanedFrom'].isna()\n",
    "valid_rows = valid_mask.sum()\n",
    "invalid_rows = combined_df_cleaned.shape[0] - valid_rows\n",
    "\n",
    "\n",
    "print(f\"üìä Initial summary of 'CleanedFrom' column cleanup (before ECE inference):\")\n",
    "print(f\"‚úÖ Valid 'CleanedFrom' values (ECE, New Enrolment, Repeater, Transferred In): {valid_rows}\")\n",
    "print(f\"‚ùå Invalid or unmapped 'CleanedFrom' values: {invalid_rows}\")\n",
    "print(f\"üìã Frequency breakdown:\")\n",
    "print(from_counts)\n",
    "\n",
    "# --- Inference from prior year ECE enrollment ---\n",
    "original_ece_count = (combined_df_cleaned['CleanedFrom'] == 'ECE').sum()\n",
    "\n",
    "# Safely calculate previous year\n",
    "start_year = int(expected_school_year.split('-')[1])\n",
    "previous_year = start_year - 1\n",
    "\n",
    "ece_last_year = df_enrolments[\n",
    "    (df_enrolments['stueClass'] == 'GK') &\n",
    "    (df_enrolments['stueYear'] == previous_year)\n",
    "]\n",
    "\n",
    "ece_lookup = set(\n",
    "    zip(\n",
    "        ece_last_year['stuGiven'].str.upper().str.strip(),\n",
    "        ece_last_year['stuFamilyName'].str.upper().str.strip(),\n",
    "        pd.to_datetime(ece_last_year['stuDoB'], errors='coerce')\n",
    "    )\n",
    ")\n",
    "\n",
    "def infer_from_ece(row):\n",
    "    if pd.notna(row['CleanedFrom']):\n",
    "        return row['CleanedFrom']\n",
    "    key = (str(row['FirstName']).upper().strip(), str(row['LastName']).upper().strip(), pd.to_datetime(row['FixedParsedBirthDate'], errors='coerce'))\n",
    "    if key in ece_lookup:\n",
    "        return 'ECE'\n",
    "    return row['CleanedFrom']\n",
    "\n",
    "combined_df_cleaned['CleanedFrom'] = combined_df_cleaned.apply(infer_from_ece, axis=1)\n",
    "\n",
    "# --- Final ECE summary ---\n",
    "ece_total = (combined_df_cleaned['CleanedFrom'] == 'ECE').sum()\n",
    "ece_inferred = ece_total - original_ece_count\n",
    "\n",
    "print(f\"\\nüß† ECE inference summary:\")\n",
    "print(f\"üü¢ Already had ECE before inference: {original_ece_count}\")\n",
    "print(f\"üîç Inferred ECE entries: {ece_inferred}\")\n",
    "print(f\"‚úÖ Total ECE entries after processing: {ece_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cf3ac-9dbb-4540-8194-8f24d01ef13b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Infer Attended ECE Status (anytime in the past or from source column)\n",
    "# Match based on FirstName, LastName, and FixedParsedBirthDate\n",
    "# If student appears in df_enrolments with stueClass == 'GK', OR has 'Yes' in original ECE/Kinder field, mark as 'Yes'\n",
    "\n",
    "# Step 1: Prepare matching key\n",
    "combined_df_cleaned['__MatchKey'] = (\n",
    "    combined_df_cleaned['FirstName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['LastName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['FixedParsedBirthDate'].astype(str)\n",
    ")\n",
    "\n",
    "df_enrolments['__MatchKey'] = (\n",
    "    df_enrolments['stuGiven'].astype(str).str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuFamilyName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuDoB'].astype(str)\n",
    ")\n",
    "\n",
    "# Step 2: Match keys from ECE history (GK class)\n",
    "ece_match_keys = set(\n",
    "    df_enrolments.loc[df_enrolments['stueClass'] == 'GK', '__MatchKey']\n",
    ")\n",
    "\n",
    "# Step 3: Create inferred column (start with None)\n",
    "combined_df_cleaned['InferredAttendedECE'] = None\n",
    "\n",
    "# Step 4: Source-based assignment\n",
    "ece_col = 'Attended \\nECE/Kinder\\n ?'\n",
    "source_yes_mask = pd.Series([False] * len(combined_df_cleaned))\n",
    "if ece_col in combined_df_cleaned.columns:\n",
    "    source_yes_mask = combined_df_cleaned[ece_col].astype(str).str.strip().str.upper().isin({'YES', 'Y'})\n",
    "    combined_df_cleaned.loc[source_yes_mask, 'InferredAttendedECE'] = 'Yes'\n",
    "\n",
    "# Step 5: Inference from enrollment history (only if not already 'Yes')\n",
    "inferred_yes_mask = (\n",
    "    combined_df_cleaned['__MatchKey'].isin(ece_match_keys) &\n",
    "    (combined_df_cleaned['InferredAttendedECE'] != 'Yes')\n",
    ")\n",
    "combined_df_cleaned.loc[inferred_yes_mask, 'InferredAttendedECE'] = 'Yes'\n",
    "\n",
    "# Step 6: Cleanup temporary key\n",
    "combined_df_cleaned.drop(columns='__MatchKey', inplace=True)\n",
    "df_enrolments.drop(columns='__MatchKey', inplace=True, errors='ignore')\n",
    "\n",
    "# Step 7: Summary\n",
    "total_students = len(combined_df_cleaned)\n",
    "source_yes = source_yes_mask.sum()\n",
    "inferred_yes = inferred_yes_mask.sum()\n",
    "final_yes = (combined_df_cleaned['InferredAttendedECE'] == 'Yes').sum()\n",
    "\n",
    "print(f\"üß† Inferred or recognized ECE attendance (any year):\")\n",
    "print(f\"üìå 'Yes' from source column '{ece_col}': {source_yes}\")\n",
    "print(f\"üîç Inferred from historical GK enrollment: {inferred_yes}\")\n",
    "print(f\"‚úÖ Total students marked as having attended ECE: {final_yes} of {total_students}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b191b4f-bb18-4835-86c4-2170de12527f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean and Infer Transferred From which school\n",
    "# Step 1: Define draft mapping (to be refined after inspecting unique values)\n",
    "transferred_from_mapping = {\n",
    "    '#REF!': 'None',\n",
    "    'A.E.S': 'Aur Elementary School',\n",
    "    'AES': 'Aur Elementary School',\n",
    "    'AHS': 'Assumption High School',\n",
    "    'Aelok Elementary School': 'Aerok Elementary School',\n",
    "    'Aelonlaplap Elementary Schools': 'Delap Elementary School',\n",
    "    'Aelonlaplap, Buoj Elementary School': 'Buoj Elementary School',\n",
    "    'Aelonlaplap, Buoj Elementary School.': 'Buoj Elementary School',\n",
    "    'Aelonlaplap, Je ES': 'Jobwan Elementary School',\n",
    "    'Aelonlaplap, Jebwon Elementary School': 'Jobwan Elementary School',\n",
    "    'Aelonlaplap, Katiej Elementary School': 'Katiej Elementary School',\n",
    "    'Aelonlaplap, Mejel Elementary School': 'Mejel Elementary School',\n",
    "    'Aelonlaplap, Woja ES.': 'Woja Elementary School (Ailinglaplap)',\n",
    "    'Aiea HS( Hawaii': 'International',\n",
    "    'Ailuk E.S.': 'Ailuk Elementary School',\n",
    "    'Ailuk Elementary School': 'Ailuk Elementary School',\n",
    "    'Airok E.S.(MAL)': 'Aerok Elementary School',\n",
    "    'Airok Elem. School(Ailinglaplap)': 'Woja Elementary School (Ailinglaplap)',\n",
    "    'Ajeltake': 'Ajeltake Elementary School',\n",
    "    'Ajeltake E.S.': 'Ajeltake Elementary School',\n",
    "    'Ajeltake Elem.': 'Ajeltake Elementary School',\n",
    "    'Ajeltake Elementary School': 'Ajeltake Elementary School',\n",
    "    'Alaska': 'International',\n",
    "    'Arkansas': 'International',\n",
    "    'Arno': 'Arno Elementary School',\n",
    "    'Arno E.S.': 'Arno Elementary School',\n",
    "    'Arno Elem. School': 'Arno Elementary School',\n",
    "    'Arno Elementary School': 'Arno Elementary School',\n",
    "    'Arno,  Ine ES.': 'Ine Elementary School',\n",
    "    'Arno, Ine ES.': 'Ine Elementary School',\n",
    "    'Assumption Elementary School': 'Assumption Elementary School',\n",
    "    'Assumption H.S.': 'Assumption High School',\n",
    "    'Aur Elem. School': 'Aur Elementary School',\n",
    "    'Aur Elementary School': 'Aur Elementary School',\n",
    "    'Aur, Tobal Elementary School': 'Tobal Elementary School',\n",
    "    'Bikareej Elementary School': 'Bikarej Elementary School',\n",
    "    'Bikarej Elem. School': 'Bikarej Elementary School',\n",
    "    'Buoj E.S.': 'Buoj Elementary School',\n",
    "    'Buoj Elem. School': 'Buoj Elementary School',\n",
    "    'Buoj Elementary School': 'Buoj Elementary School',\n",
    "    'COHS': 'Majuro Coop High School',\n",
    "    'COOP': 'Majuro Coop High School',\n",
    "    'COOP HS': 'Majuro Coop High School',\n",
    "    'Calvary High School': 'Ebeye Calvary High School',\n",
    "    'Carfield Elem. USA': 'International',\n",
    "    'Carter HS': 'International',\n",
    "    'Carter HS (USA)': 'International',\n",
    "    'Cascade Middle SChool': 'International',\n",
    "    'Cascade Middle School': 'International',\n",
    "    'Castle High, HI': 'International',\n",
    "    'Coop': 'Majuro Coop High School',\n",
    "    'Coop HS': 'Majuro Coop High School',\n",
    "    'DES': 'Delap Elementary School',\n",
    "    'Deaf Center': 'Ebeye Deaf Center Primary',\n",
    "    'Delap Elem. School': 'Delap Elementary School',\n",
    "    'Delap Elementary School': 'Delap Elementary School',\n",
    "    'Dropped-out': 'None',\n",
    "    'ECES': 'Ebeye Christian Elementary School',\n",
    "    'EPES': 'Ebeye Public Elementary School',\n",
    "    'EPMS': 'Ebeye Public Middle School',\n",
    "    'EPSS, Ebeye': 'Ebeye Public Elementary School',\n",
    "    'Ebeye': 'Ebeye Public Elementary School',\n",
    "    'Ebeye Calvary Elementary School': 'Ebeye Calvary Elementary School',\n",
    "    'Ebeye Calvary H.S.': 'Ebeye Calvary High School',\n",
    "    'Ebeye E. School': 'Ebeye Public Elementary School',\n",
    "    'Ebeye Elem. School': 'Ebeye Public Elementary School',\n",
    "    'Ebeye Elem.School': 'Ebeye Public Elementary School',\n",
    "    'Ebeye Elementary School': 'Ebeye Public Elementary School',\n",
    "    'Ebeye Middle Scool': 'Ebeye Public Middle School',\n",
    "    'Ebeye Public E. School': 'Ebeye Public Middle School',\n",
    "    'Ebeye Public E.S.': 'Ebeye Public Middle School',\n",
    "    'Ebeye Public Elementary School': 'Ebeye Public Elementary School',\n",
    "    'Ebeye SDA': 'Ebeye SDA Elementary School',\n",
    "    'Ebeye, ECES': 'Ebeye Christian Elementary School',\n",
    "    'Ebeye, PSS': 'Ebeye SDA High School',\n",
    "    'Ebeye, SDA': 'Ebeye SDA Elementary School',\n",
    "    'Ebezon Elem PI': 'International',\n",
    "    'Ebon E.S.': 'Ebeye Public Elementary School',\n",
    "    'Ebon Elem. School': 'Ebon Elementary School',\n",
    "    'Ebon, Eneko Ion Elementary School': 'Enekoion Elementary School',\n",
    "    'Ejit Elem. School': 'Ejit Elementary School',\n",
    "    'Ejit Elementary School': 'Ejit Elementary School',\n",
    "    'Ejit Is. Elem School': 'Ejit Elementary School',\n",
    "    'Enejet Elementary School': 'Enejet Elementary School',\n",
    "    'Enejet Elemntary School': 'Enejet Elementary School',\n",
    "    'Enekion Elem Ebon': 'Enekoion Elementary School',\n",
    "    'Enewetak ES': 'Enewetak Elementary School',\n",
    "    'Enewetak Elem.': 'Enewetak Elementary School',\n",
    "    'Enewetak Elementary School': 'Enewetak Elementary School',\n",
    "    'FIFe HS (WA State)': 'International',\n",
    "    'Father Hacker H.S.': 'Father Hacker High School',\n",
    "    'Fife HS, WA State': 'International',\n",
    "    'Gateway Elem. AZ': 'International',\n",
    "    'Georgia Middle USA': 'International',\n",
    "    'HI': 'International',\n",
    "    'Hawaii': 'International',\n",
    "    'Hillside Elem. WA': 'International',\n",
    "    'Hilo High School': 'Jabro High School',\n",
    "    'Hilo Middle  Hilo': 'International',\n",
    "    'Ine Elem.': 'Ine Elementary School',\n",
    "    'Ine Elementary SChool': 'Ine Elementary School',\n",
    "    'Ine Elementary School': 'Ine Elementary School',\n",
    "    'JHS': 'Jaluit High School',\n",
    "    'JPS': 'None',\n",
    "    'Jabor E.S.': 'Jabor Elementary School',\n",
    "    'Jabor Elementary School': 'Jabor Elementary School',\n",
    "    'Jabro HS (EBEYE)': 'Jabro High School',\n",
    "    'Jabro Private School': 'Jabro High School',\n",
    "    'Jabro School': 'Jabro High School',\n",
    "    'Jaluit Elem. School': 'Jaluit Elementary School',\n",
    "    'Jaluit High School': 'Jaluit High School',\n",
    "    'Jaluit Saint Joseph Elementary School': 'St. Joseph Elementary School',\n",
    "    'Jaluit, Mejrirok Elementary School': 'Mejrirok Elementary School',\n",
    "    'Jang Elementary School': 'Jang Elementary School',\n",
    "    'Japo Arno Elem.': 'Japo Elementary School',\n",
    "    'Japo Elementary School': 'Japo Elementary School',\n",
    "    'Jeh Elem. School': 'Jeh Elementary School',\n",
    "    'Jeh Elementary School': 'Jeh Elementary School',\n",
    "    'Jepo Elem. School': 'Japo Elementary School',\n",
    "    'Jobwan Elemntary School': 'Jobwan Elementary School',\n",
    "    'KAHS': 'Kwajalein Atoll High School',\n",
    "    'Kalenia Ole Elem. HI': 'International',\n",
    "    'Kamaikin High, WA': 'International',\n",
    "    'Kaven Elementary School': 'Kaven Elementary School',\n",
    "    'Kilange Elementary School': 'Kilange Elementary School',\n",
    "    'Kili E.S.': 'Kili Elementary School',\n",
    "    'Kili Elem.': 'Kili Elementary School',\n",
    "    'Kili Elementary School': 'Kili Elementary School',\n",
    "    'Kili Island Elementary School': 'Kili Elementary School',\n",
    "    'Kwajalein Atoll H.S.': 'Kwajalein Atoll High School',\n",
    "    'Kwajalein Atoll High School': 'Kwajalein Atoll High School',\n",
    "    'LES': 'Lukonwod Elementary School',\n",
    "    'LHS': 'Laura High School',\n",
    "    'LIES': 'Long Island Elementary School',\n",
    "    'LSA': 'Life Skills Academy',\n",
    "    'Lae Elementary School': 'Lae Elementary School',\n",
    "    'Laura Elementary School': 'Laura Elementary School',\n",
    "    'Laura H.S.': 'Laura High School',\n",
    "    'Light House': 'Lighthouse Apostolic Academy',\n",
    "    'Likiep Elem. School': 'Likiep Elementary School',\n",
    "    'Long Isl. Elementary School': 'Long Island Elementary School',\n",
    "    'Long Island E.S.': 'Long Island Elementary School',\n",
    "    'Long Island Elementary School': 'Long Island Elementary School',\n",
    "    'Longar Elementary School': 'Longar Elementary School',\n",
    "    'Lonng Island Elem. School': 'Long Island Elementary School',\n",
    "    'Lukoj Elem. School': 'Lukoj Elementary School',\n",
    "    'Lukwonwod Elem.': 'Lukonwod Elementary School',\n",
    "    'MBCA': 'International',\n",
    "    'MCHS': 'Marshall Christians High School',\n",
    "    'MIHS': 'Marshall Islands High School',\n",
    "    'MMS': 'Majuro Middle School',\n",
    "    'Mae Elementary SChool': 'Mae Elementary School',\n",
    "    'Majkin Elementary school': 'Majkin Elementary School',\n",
    "    'Majuro': 'None',\n",
    "    'Majuro Baptist Christian Academy': 'Laura Christian Academy',\n",
    "    'Majuro Middle School': 'Majuro Middle School',\n",
    "    'Majuro Midle School': 'Majuro Middle School',\n",
    "    'Majuro SDA': 'Delap SDA Elementary School',\n",
    "    'Majuro, Ajeltake Elementary School': 'Ajeltake Elementary School',\n",
    "    'Majuro, Assumption Primary School': 'None',\n",
    "    'Majuro, Delap Elementary School': 'Majuro Coop Elementary School',\n",
    "    'Majuro, Ejit Elementary School': 'Majuro Baptist Elementary School',\n",
    "    'Majuro, Long Island Elementary School': 'Long Island Elementary School',\n",
    "    'Majuro, MBCA Primary School': 'None',\n",
    "    'Majuro, North Delap Elementary School': 'North Delap Elementary School',\n",
    "    'Majuro, Rairok Rainbow Elementary School': 'Majuro Coop Elementary School',\n",
    "    'Majuro, SDA Primary School': 'None',\n",
    "    'Makapala Elem. HI': 'International',\n",
    "    'Maloelap': 'Aerok Elementary School',\n",
    "    'Maloelap Elementary Schools': 'Aerok Elementary School',\n",
    "    'Marshall Christian Elem. School': 'Marshall Christians High School',\n",
    "    'Marshall Christian High School': 'Marshall Christians High School',\n",
    "    'Marshall Islands H.S': 'Marshall Islands High School',\n",
    "    'Marshall Islands High School': 'Marshall Islands High School',\n",
    "    'Matoleen Elementary School': 'Matolen Elementary School',\n",
    "    'Matolen Elementary School': 'Matolen Elementary School',\n",
    "    'Maui HS (Hawaii)': 'International',\n",
    "    'Maui High, HI': 'International',\n",
    "    'McKinley, HI': 'International',\n",
    "    'Mckay HS': 'International',\n",
    "    'Mejatto E.S.': 'Mejatto Elementary School',\n",
    "    'Mejatto Elem.': 'Mejatto Elementary School',\n",
    "    'Mejatto Elem.School': 'Mejatto Elementary School',\n",
    "    'Mejit Elem. School': 'Mejit Elementary School',\n",
    "    'Mejit Elementary School': 'Mejit Elementary School',\n",
    "    'Mejjato Elementary School': 'Mejatto Elementary School',\n",
    "    'Mejrirok E.S.': 'Mejrirok Elementary School',\n",
    "    'Mejrirok Elementary SChool': 'Mejrirok Elementary School',\n",
    "    'Mejrirok Elementary School': 'Mejrirok Elementary School',\n",
    "    'Mili Elem.': 'Mili Elementary School',\n",
    "    'Mili Elem. School': 'Mili Elementary School',\n",
    "    'Mili Elem.School': 'Mili Elementary School',\n",
    "    'Mili Elementary School': 'Mili Elementary School',\n",
    "    'Mili elementary School': 'Mili Elementary School',\n",
    "    'Mili, Tokewa Elem.': 'Tokewa Elementary School',\n",
    "    'N Middle School WA': 'International',\n",
    "    'NDES': 'North Delap Elementary School',\n",
    "    'NIHS': 'Northern Islands High School',\n",
    "    'Nallo Elem. School': 'Nallo Elementary School',\n",
    "    'Nallo Elementary School': 'Nallo Elementary School',\n",
    "    'Nallu Elem.School': 'Nallo Elementary School',\n",
    "    'Namdrik': 'Namdrik Elementary School',\n",
    "    'Namdrik Elementary School': 'Namdrik Elementary School',\n",
    "    'Namo': 'Namu Elementary School',\n",
    "    'Namu Elem.School': 'Namu Elementary School',\n",
    "    'Namu Elementary School': 'Namu Elementary School',\n",
    "    'Narmij Elem Jaluit': 'Narmij Elementary School',\n",
    "    'Narmij Elementary School': 'Narmij Elementary School',\n",
    "    'Niu Valley Elem. HI': 'International',\n",
    "    'North Carolina USA': 'International',\n",
    "    'North Delap Elem. School': 'North Delap Elementary School',\n",
    "    'North Delap Elem.School': 'North Delap Elementary School',\n",
    "    'North Delap Elementary School': 'North Delap Elementary School',\n",
    "    'Ollet Elementary School': 'Ollet Elementary School',\n",
    "    'PPS - Ailinglaplap': 'None',\n",
    "    'PSS -jaluit': 'Jaluit Elementary School',\n",
    "    'Pahoa HS. HILO': 'International',\n",
    "    'Phillippines': 'International',\n",
    "    'QPS': 'Queen of Peace Elementary School',\n",
    "    'RES': 'Rita Elementary School',\n",
    "    'RRES': 'RonRon Protestant Elementary School',\n",
    "    'Rairok E.S.': 'Rairok Elementary School',\n",
    "    'Rairok Elem. School': 'Rairok Elementary School',\n",
    "    'Rairok Elementary School': 'Rairok Elementary School',\n",
    "    'Rita E.S.': 'Rita Elementary School',\n",
    "    'Rita Elem. School': 'Rita Elementary School',\n",
    "    'Rita Elementary School': 'Rita Elementary School',\n",
    "    'Rongrong E.S.': 'RonRon Protestant Elementary School',\n",
    "    'SDA': 'None',\n",
    "    'SDA HS': 'None',\n",
    "    'Santo PSS': 'None',\n",
    "    'St.Joseph Academy': 'St. Joseph Elementary School',\n",
    "    'TPS': 'None',\n",
    "    'Tarawa Elem.School': 'Tarawa Elementary School',\n",
    "    'Texas Hs. USA': 'International',\n",
    "    'Tinak Arno Elem.': 'Tinak Elementary School',\n",
    "    'Tinak Elem. School': 'Tinak Elementary School',\n",
    "    'Tinak Elementary School': 'Tinak Elementary School',\n",
    "    'Tobal Aur Elem.': 'Tobal Elementary School',\n",
    "    'Toka Elementary School': 'Toka Elementary School',\n",
    "    'Toka elementary School': 'Toka Elementary School',\n",
    "    'Tokewa Elementary School': 'Tokewa Elementary School',\n",
    "    'Tutu Elementary School': 'Tutu Elementary School',\n",
    "    'U,S': 'International',\n",
    "    'U.S': 'International',\n",
    "    'U.S Logan Middle School': 'International',\n",
    "    'U.S.A': 'International',\n",
    "    'USA': 'International',\n",
    "    'Ujae Elem. School': 'Ujae Elementary School',\n",
    "    'Ujae Elementary School': 'Ujae Elementary School',\n",
    "    'Ulien Elem. School': 'Ulien Elementary School',\n",
    "    'Ulien Elementary School': 'Ulien Elementary School',\n",
    "    'Utrok Elem.School': 'Utrik Elementary School',\n",
    "    'WES, WA': 'International',\n",
    "    'WPES': 'None',\n",
    "    'WPES, Wotje': 'None',\n",
    "    'WSHS': 'None',\n",
    "    'Washington, USA': 'International',\n",
    "    'Webling Elem. HI': 'International',\n",
    "    'Wichita Elem KS': 'International',\n",
    "    'Woja Ailinlaplap Elem.': 'Woja Elementary School (Ailinglaplap',\n",
    "    'Woja Elem.': 'Woja Elementary School (Majuro)',\n",
    "    'Woja Elem. School': 'Woja Elementary School (Majuro)',\n",
    "    'Woja Elem.School (Majuro)': 'Woja Elementary School (Majuro)',\n",
    "    'Wotho Elem.School': 'Wotho Elementary School',\n",
    "    'Wotje E.S.': 'Wotje Elementary School',\n",
    "    'Wotje Elementary School': 'Wotje Elementary School',\n",
    "    'Wotje, Wotje Elementary School': 'Wotje Elementary School',\n",
    "    'Xavier': 'International',\n",
    "    'YSP': 'International',\n",
    "    'from Catholic': 'International',\n",
    "    'from Hawaii': 'International',\n",
    "    'from Lae': 'Lae Elementary School',\n",
    "    'late registered': 'None',\n",
    "    '': np.nan,\n",
    "    # Add more mappings after reviewing the unique values below...\n",
    "}\n",
    "\n",
    "# Step 2: Apply the mapping to source column\n",
    "combined_df_cleaned['InferredTransferredFromWhichSchool'] = (\n",
    "    combined_df_cleaned['Transferred\\nFROM']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace('', np.nan)\n",
    "    .map(transferred_from_mapping)\n",
    ")\n",
    "\n",
    "# Step 3: Show unique raw values for mapping refinement\n",
    "# print(\"üìã Unique values in 'Transferred\\\\nFROM':\")\n",
    "# unique_transferred_from_values = (\n",
    "#     combined_df_cleaned['Transferred\\nFROM']\n",
    "#     .dropna()\n",
    "#     .astype(str)\n",
    "#     .str.strip()\n",
    "#     .unique()\n",
    "# )\n",
    "# for val in sorted(unique_transferred_from_values, key=lambda x: x.lower()):\n",
    "#     print(f\" - '{val}'\")\n",
    "\n",
    "# Step 4: Infer from previous year's enrolment if not already set\n",
    "start_year = int(expected_school_year.split('-')[1])\n",
    "previous_year = start_year - 1\n",
    "\n",
    "# Create student match key\n",
    "combined_df_cleaned['__MatchKey'] = (\n",
    "    combined_df_cleaned['FirstName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['LastName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['FixedParsedBirthDate'].astype(str)\n",
    ")\n",
    "df_enrolments['__MatchKey'] = (\n",
    "    df_enrolments['stuGiven'].astype(str).str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuFamilyName'].astype(str).str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuDoB'].astype(str)\n",
    ")\n",
    "\n",
    "# Create a lookup from last year‚Äôs enrolments with their school number\n",
    "df_last_year = df_enrolments[df_enrolments['stueYear'] == previous_year][['__MatchKey', 'schNo']].drop_duplicates()\n",
    "last_year_school_lookup = dict(zip(df_last_year['__MatchKey'], df_last_year['schNo']))\n",
    "\n",
    "# Infer where not already set\n",
    "mask_missing = combined_df_cleaned['InferredTransferredFromWhichSchool'].isna()\n",
    "inferred_values = combined_df_cleaned.loc[mask_missing, '__MatchKey'].map(last_year_school_lookup)\n",
    "\n",
    "# Only keep inference if it's from a *different* school than current\n",
    "inferred_diff_school = inferred_values[\n",
    "    inferred_values != combined_df_cleaned.loc[mask_missing, 'Corrected_schNo']\n",
    "]\n",
    "\n",
    "combined_df_cleaned.loc[inferred_diff_school.index, 'InferredTransferredFromWhichSchool'] = inferred_diff_school\n",
    "\n",
    "# Step 5: Cleanup\n",
    "combined_df_cleaned.drop(columns='__MatchKey', inplace=True)\n",
    "df_enrolments.drop(columns='__MatchKey', inplace=True, errors='ignore')\n",
    "\n",
    "# Step 6: Summary\n",
    "total_cleaned = combined_df_cleaned['InferredTransferredFromWhichSchool'].notna().sum()\n",
    "total_rows = combined_df_cleaned.shape[0]\n",
    "total_inferred = inferred_diff_school.notna().sum()\n",
    "total_cleaned_only = total_cleaned - total_inferred\n",
    "\n",
    "print(\"üìä Summary of 'InferredTransferredFromWhichSchool' generation:\")\n",
    "print(f\"üßº Cleaned from source: {total_cleaned_only}\")\n",
    "print(f\"üß† Inferred from previous enrolments: {total_inferred}\")\n",
    "print(f\"‚úÖ Total populated: {total_cleaned} out of {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c49e1-8698-4da0-add4-4638ffa31e0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "# Step 1: Get unique raw values\n",
    "raw_transfer_values = (\n",
    "    combined_df_cleaned['Transferred\\nFROM']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# Step 2: Reference: official school names and acronyms\n",
    "school_codes_df = pd.DataFrame(core_lookups['schoolCodes'])  # Convert list of dicts to DataFrame\n",
    "school_codes_df['N'] = school_codes_df['N'].astype(str).str.strip()\n",
    "school_codes_df['C'] = school_codes_df['C'].astype(str).str.strip()\n",
    "\n",
    "official_school_names = school_codes_df['N'].unique().tolist()\n",
    "\n",
    "# Build acronym mapping: clean acronyms for easy lookup (e.g., LHS ‚Üí Laura High School)\n",
    "known_acronym_mapping = {}\n",
    "for _, row in school_codes_df.iterrows():\n",
    "    name = row['N']\n",
    "    acronym = ''.join([word[0] for word in name.split() if word[0].isalpha()]).upper()\n",
    "    if len(acronym) >= 2:  # Keep only meaningful acronyms\n",
    "        known_acronym_mapping[acronym] = name\n",
    "\n",
    "# Step 3: Enhanced draft mapping logic\n",
    "first_draft_mapping = {}\n",
    "\n",
    "for raw_value in raw_transfer_values:\n",
    "    val = raw_value.strip()\n",
    "    val_upper = val.upper()\n",
    "\n",
    "    # Rule 1: If contains known US/state keywords ‚Üí mark as 'International'\n",
    "    if any(x in val_upper for x in ['USA', 'U.S', 'AMERICA', 'CALIFORNIA', 'OREGON', 'HAWAII', 'NEW YORK',\n",
    "                                    'TEXAS', 'ARIZONA', 'GUAM', 'WASHINGTON', 'ALASKA', 'PHILIPPINES', 'NORTH CAROLINA']):\n",
    "        first_draft_mapping[raw_value] = 'International'\n",
    "        continue\n",
    "\n",
    "    # Rule 2: If ends with 2-letter US state abbreviation ‚Üí 'International'\n",
    "    words = val_upper.split()\n",
    "    if words and words[-1] in {\n",
    "        'HI', 'WA', 'CA', 'NY', 'TX', 'AZ', 'OR', 'GU', 'AK', 'NC'\n",
    "    }:\n",
    "        first_draft_mapping[raw_value] = 'International'\n",
    "        continue\n",
    "\n",
    "    # Rule 3: Dot-separated acronym (e.g. A.E.S ‚Üí AES ‚Üí match)\n",
    "    clean_acronym = val_upper.replace('.', '')\n",
    "    if clean_acronym in known_acronym_mapping:\n",
    "        first_draft_mapping[raw_value] = known_acronym_mapping[clean_acronym]\n",
    "        continue\n",
    "\n",
    "    # Rule 4: Comma-separated location and acronym (e.g., \"Ebeye, ECES\")\n",
    "    if ',' in val_upper:\n",
    "        parts = [p.strip() for p in val_upper.split(',')]\n",
    "        if len(parts) == 2:\n",
    "            _, possible_acronym = parts\n",
    "            possible_acronym_clean = possible_acronym.replace('.', '')\n",
    "            if possible_acronym_clean in known_acronym_mapping:\n",
    "                first_draft_mapping[raw_value] = known_acronym_mapping[possible_acronym_clean]\n",
    "                continue\n",
    "\n",
    "    # Rule 5: Exact acronym match (e.g., LHS)\n",
    "    if val_upper in known_acronym_mapping:\n",
    "        first_draft_mapping[raw_value] = known_acronym_mapping[val_upper]\n",
    "        continue\n",
    "\n",
    "    # Rule 6: Fuzzy match against official school names\n",
    "    best_match = difflib.get_close_matches(val, official_school_names, n=1, cutoff=0.75)\n",
    "    if best_match:\n",
    "        first_draft_mapping[raw_value] = best_match[0]\n",
    "    else:\n",
    "        first_draft_mapping[raw_value] = None  # unresolved\n",
    "\n",
    "# Step 4: Print for review\n",
    "print(\"üß† First Draft Mapping for 'Transferred FROM':\")\n",
    "for k in sorted(first_draft_mapping):\n",
    "    print(f\"'{k}' ‚Üí '{first_draft_mapping[k]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48737ae8-f3c1-4ad3-a1c6-423a8e015579",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cleanup Ethnicities\n",
    "\n",
    "# Step 1: Define initial mapping (you can expand this later)\n",
    "ethnicity_mapping = {\n",
    "    '-': 'Marshallese',\n",
    "    'AFRIKAANS': 'Afrikaners',\n",
    "    'CHINESE': 'Chinese',\n",
    "    'Chinese': 'Chinese',\n",
    "    'ENGLISH': 'Marshallese',\n",
    "    'ENGLISH,ARSHALLESE': 'Marshallese',\n",
    "    'ENGLISH,PALAUAN,MARSHALLESE': 'Marshallese',\n",
    "    'ENGLISH/CHICHWA': 'Marshallese',\n",
    "    'ENGLISH/CHINESE': 'Chinese',\n",
    "    'ENGLISH/FIJIAN': 'Marshallese',\n",
    "    'ENGLISH/FILIPINO': 'Filipino',\n",
    "    'ENGLISH/KIRIBATI': 'Kiribatese',\n",
    "    'ENGLISH/MARSHALLESE': 'Marshallese',\n",
    "    'ENGLISH/MARSHALLESE/GILBERTESE': 'Marshallese',\n",
    "    'ENGLISH/MARSHALLESE/KOREAN': 'Marshallese',\n",
    "    'ENGLISH/MARSHALLESE/TUVALUAN': 'Marshallese',\n",
    "    'ENGLISH/PIJIN': 'Solomon Islander',\n",
    "    'ENGLISH/TAGALOG': 'Filipino',\n",
    "    'ENGLISH/TAGALOG/ILOMGGO': 'Filipino',\n",
    "    'ENGLISH/TUVALUAN': 'Tuvaluan',\n",
    "    'ENGLISH/URDU': 'Pakistani',\n",
    "    'FIJIAN': 'Fijian',\n",
    "    'FIJIAN/TONGAN': 'Fijian',\n",
    "    'FILIPINO': 'Filipino',\n",
    "    'GERMAN/SPANISH': 'German',\n",
    "    'I-KIRIBATI': 'Kiribatese',\n",
    "    'JAPANASE': 'Japanese', \n",
    "    'JAPANESE': 'Japanese',\n",
    "    'KIRIBATI': 'Kiribatese',\n",
    "    'KIRIBATI/ENGLISH': 'Kiribatese',\n",
    "    'KOREAN': 'Korean',\n",
    "    'LATIN': 'Other',\n",
    "    'MALAWIAN': 'Malawian',\n",
    "    'MARSHALLESE': 'Marshallese',\n",
    "    'Marshallese': 'Marshallese',\n",
    "    'MARSHALLESE/AMERICAN': 'Marshallese',\n",
    "    'MARSHALLESE/CHINESE': 'Marshallese',\n",
    "    'MARSHALLESE/ENGLISH': 'Marshallese',\n",
    "    'MARSHALLESE/ENGLISH/FIJIAN': 'Marshallese',\n",
    "    'MARSHALLESE/FIJIAN': 'Marshallese',\n",
    "    'MARSHALLESE/FILIPINO': 'Marshallese',\n",
    "    'MARSHALLESE/KIRIBATI': 'Marshallese',\n",
    "    'MARSHALLESE/KIWI': 'Marshallese',\n",
    "    'MARSHALLESE/KOSRAEN': 'Marshallese',\n",
    "    'MARSHALLESE/NGLISH': 'Marshallese',\n",
    "    'MARSHALLESE/POHNPEAIN/ENGLISH': 'Marshallese',\n",
    "    'MARSHALLESE/POHNPEIAN/AMERICAN-ITALIAN': 'Marshallese',\n",
    "    'MARSHALLESE/POHNPEIN/JAPANESE': 'Marshallese',\n",
    "    'MARSHALLESE/YAPESE': 'Marshallese',\n",
    "    'NEPALI': 'Nepalese',\n",
    "    'NIGERIAN (HAVSA)': 'Nigerien',\n",
    "    'POHNPEI/MARSHALLESE/US': 'Marshallese',\n",
    "    'ROTUMAN/FIJIAN': 'Fijian',\n",
    "    'Solomon Islander': 'Solomon Islander',\n",
    "    'SOLOMON ISLANDER': 'Solomon Islander',\n",
    "    'SOUTH AFRICAN': 'South African',\n",
    "    'TAIWANESE': 'Taiwanese',\n",
    "    'TAIWANESE/MARSHALLESE': 'Marshallese',\n",
    "    'TONGAN': 'Tongan',\n",
    "    'TUVALU/ENGLISH/MARSHALLESE': 'Marshallese',\n",
    "    'TUVALUAN': 'Tuvaluan',\n",
    "    '': 'Marshallese',\n",
    "    'nan': 'Marshallese',\n",
    "    'NaN': 'Marshallese',\n",
    "    'None': 'Marshallese',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Step 2: Apply the mapping\n",
    "combined_df_cleaned['CleanedEthnicity'] = combined_df_cleaned['Ethnicity'].astype(str).map(ethnicity_mapping)\n",
    "\n",
    "# Step 3: Show unique raw values for inspection\n",
    "unique_ethnicities = combined_df_cleaned['Ethnicity'].dropna().astype(str).unique()\n",
    "\n",
    "print(\"üìã Unique values in 'Ethnicity':\")\n",
    "for val in sorted(unique_ethnicities, key=lambda x: x.lower()):\n",
    "    print(f\" - '{val}'\")\n",
    "\n",
    "# Step 4: Get valid ethnicity values from core_lookups\n",
    "valid_ethnicities = {entry['N'] for entry in core_lookups['ethnicities']}\n",
    "\n",
    "# Step 5: Count how many cleaned values match a valid one\n",
    "ethnicity_counts = combined_df_cleaned['CleanedEthnicity'].value_counts(dropna=False)\n",
    "valid_ethnicity_rows = combined_df_cleaned['CleanedEthnicity'].isin(valid_ethnicities).sum()\n",
    "total_ethnicity_rows = combined_df_cleaned.shape[0]\n",
    "invalid_ethnicity_rows = total_ethnicity_rows - valid_ethnicity_rows\n",
    "\n",
    "# Step 6: Summary\n",
    "print(f\"\\nüìä Summary of 'Ethnicity' column cleanup:\")\n",
    "print(f\"‚úÖ Valid 'CleanedEthnicity' values (from core_lookups['ethnicities']): {valid_ethnicity_rows}\")\n",
    "print(f\"‚ùå Invalid or unmapped 'CleanedEthnicity' values: {invalid_ethnicity_rows}\")\n",
    "print(f\"üìã Frequency breakdown:\")\n",
    "print(ethnicity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235895db-eeff-4f5a-abf7-b5263cf42550",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean citizenship\n",
    "\n",
    "# Step 1: Define initial mapping for common cases and typos\n",
    "citizenship_mapping = {\n",
    "    '-': 'Marshall Islands',\n",
    "    'AMERICAN': 'USA',\n",
    "    'AUSTRALIAN/KIWI': 'Australia',\n",
    "    'BLACK AFRICANS': np.nan,\n",
    "    'CHINA': 'China',\n",
    "    'CHINESE': 'China',\n",
    "    'EUROPEAN': np.nan,\n",
    "    'FIJI': 'Fiji',\n",
    "    'FIJIAN': 'Fiji',\n",
    "    'FIJIAN/TONGAN': 'Fiji',\n",
    "    'FILILPINO': 'Philippines',\n",
    "    'FILIPINO': 'Philippines',\n",
    "    'I-KIRIBATI': 'Kiribati',\n",
    "    'ITALIAN': 'Italy',\n",
    "    'JAPAN': 'Japan',\n",
    "    'JAPANESE': 'Japan',\n",
    "    'KIRIBATI': 'Kiribati',\n",
    "    'KIRIBATI/SOLOMON ISLANDS': 'Kiribati',\n",
    "    'KIRIBATI/TUVALU': 'Kiribati',\n",
    "    'KOREA': 'South Korea',\n",
    "    'MALAWI': 'Malawi',\n",
    "    'MALAWIAN': 'Malawi',\n",
    "    'MARSHALESE': 'Marshall Islands',\n",
    "    'MARSHALESE/FIJIAN': 'Marshall Islands',\n",
    "    'MARSHALLESE': 'Marshall Islands',\n",
    "    'MARSHALLESE/AMERICAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/AMERICAN/CHUUKESE/JAPANESE/HAWAIIN': 'Marshall Islands',\n",
    "    'MARSHALLESE/AMERICAN/JAPANESE': 'Marshall Islands',\n",
    "    'MARSHALLESE/CHUUKESE,FILIPINO,YAPESE': 'Marshall Islands',\n",
    "    'MARSHALLESE/FIJIAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/FILIPINA': 'Marshall Islands',\n",
    "    'MARSHALLESE/FILIPINO': 'Marshall Islands',\n",
    "    'MARSHALLESE/HAWAIIAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/JAPANESE/ITALIAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/KIRIBATI': 'Marshall Islands',\n",
    "    'MARSHALLESE/KIRIBATI/TUVALU': 'Marshall Islands',\n",
    "    'MARSHALLESE/KIRIBATI/TUVALUAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/KIWI': 'Marshall Islands',\n",
    "    'MARSHALLESE/KOREAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/LATINA': 'Marshall Islands',\n",
    "    'MARSHALLESE/NZ': 'Marshall Islands',\n",
    "    'MARSHALLESE/PALAUAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/PALAUAN,GERMAN,CHAMORRO,KOREAN,FILIPINO,PALAU': 'Marshall Islands',\n",
    "    'MARSHALLESE/POHNPEAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/POHNPEI': 'Marshall Islands',\n",
    "    'MARSHALLESE/POHNPEIAN/AMERICAN/ITALIAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/SAMOAN': 'Marshall Islands',\n",
    "    'MARSHALLESE/TAIWANESE': 'Marshall Islands',\n",
    "    'MARSHALLESE/YAPESE': 'Marshall Islands',\n",
    "    'MMARSHALLESE/KIWI': 'Marshall Islands',\n",
    "    'NEPALI': 'Nepal',\n",
    "    'NIGERIA': 'Niger',\n",
    "    'PACIFIC ISLANDER': 'Other Pacific Island',\n",
    "    'PAKISTANI': 'Pakistan',\n",
    "    'PAPUA NEW GUINEA': 'Papua NEw Guinea',\n",
    "    'PHP': 'Philippines',\n",
    "    'RMI': 'Marshall Islands',\n",
    "    'RMI/NZ': 'Marshall Islands',\n",
    "    'RMI/USA': 'Marshall Islands',\n",
    "    'SOLOMON ISLANDER': 'Solomon Islands',\n",
    "    'SOLOMON ISLANDER/I-KIRIBATI': 'Solomon Islands',\n",
    "    'SOLOMON ISLANDS': 'Solomon Islands',\n",
    "    'SOUTH AFRICAN': np.nan,\n",
    "    'TONGAN': 'Tonga',\n",
    "    'TUVALU': 'Tuvalu',\n",
    "    'TUVALUAN': 'Tuvalu',\n",
    "    'US': 'USA',\n",
    "    'USA': 'USA',\n",
    "    'USA/RMI': 'Marshall Islands',\n",
    "    '': 'Marshall Islands',\n",
    "    'None': 'Marshall Islands',\n",
    "    'NaN': 'Marshall Islands',\n",
    "    'nan': 'Marshall Islands',\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# Step 2: Apply mapping to a new standardized column\n",
    "combined_df_cleaned['CleanedCitizenship'] = combined_df_cleaned['Citizenship'].astype(str).map(citizenship_mapping)\n",
    "\n",
    "# Step 3: Show unique raw values in original column\n",
    "unique_citizenship_values = combined_df_cleaned['Citizenship'].dropna().astype(str).unique()\n",
    "\n",
    "print(\"üìã Unique values in 'Citizenship':\")\n",
    "for val in sorted(unique_citizenship_values, key=lambda x: x.lower()):\n",
    "    print(f\" - '{val}'\")\n",
    "\n",
    "# Step 4: Get valid official values from core_lookups['citizenships']\n",
    "valid_citizenships = {entry['N'] for entry in core_lookups['nationalities']}\n",
    "\n",
    "# Step 5: Count how many rows match valid citizenships\n",
    "citizenship_counts = combined_df_cleaned['CleanedCitizenship'].value_counts(dropna=False)\n",
    "valid_citizenship_rows = combined_df_cleaned['CleanedCitizenship'].isin(valid_citizenships).sum()\n",
    "total_citizenship_rows = combined_df_cleaned.shape[0]\n",
    "invalid_citizenship_rows = total_citizenship_rows - valid_citizenship_rows\n",
    "\n",
    "# Step 6: Summary\n",
    "print(f\"\\nüìä Summary of 'Citizenship' column cleanup:\")\n",
    "print(f\"‚úÖ Valid 'CleanedCitizenship' values (from core_lookups['citizenships']): {valid_citizenship_rows}\")\n",
    "print(f\"‚ùå Invalid or unmapped 'CleanedCitizenship' values: {invalid_citizenship_rows}\")\n",
    "print(f\"üìã Frequency breakdown:\")\n",
    "print(citizenship_counts)\n",
    "\n",
    "# Step 7: Show a sample of invalid rows\n",
    "invalid_citizenship_df = combined_df_cleaned[\n",
    "    ~combined_df_cleaned['CleanedCitizenship'].isin(valid_citizenships)\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Citizenship' values:\")\n",
    "display(invalid_citizenship_df[['SchoolName', 'FirstName', 'LastName', 'Citizenship', 'CleanedCitizenship']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53afe11-23b7-47d3-99c2-521fe0da7928",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clean 'Special Education Student'\n",
    "\n",
    "# Step 1: Define mapping\n",
    "sped_mapping = {\n",
    "    'SPED': 'Yes',\n",
    "    'YES': 'Yes',\n",
    "    'Yes': 'Yes',\n",
    "    '': np.nan,\n",
    "    'None': np.nan,\n",
    "    'nan': np.nan,\n",
    "    'NaN': np.nan\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "\n",
    "# Step 2: Apply mapping to a new standardized column\n",
    "combined_df_cleaned['CleanedSpEdStudent'] = combined_df_cleaned['Special\\n Education\\n Student'].astype(str).str.strip().map(sped_mapping)\n",
    "\n",
    "# Step 3: Get valid official values (in this case only 'Yes' is valid)\n",
    "valid_sped_values = {'Yes'}\n",
    "\n",
    "# Step 4: Count how many rows match valid values\n",
    "sped_counts = combined_df_cleaned['CleanedSpEdStudent'].value_counts(dropna=False)\n",
    "valid_sped_rows = combined_df_cleaned['CleanedSpEdStudent'].isin(valid_sped_values).sum()\n",
    "total_sped_rows = combined_df_cleaned.shape[0]\n",
    "invalid_sped_rows = total_sped_rows - valid_sped_rows\n",
    "\n",
    "# Step 5: Summary\n",
    "print(f\"\\nüìä Summary of 'Special Education Student' column cleanup:\")\n",
    "print(f\"‚úÖ Valid 'CleanedSpEdStudent' values (only 'Yes'): {valid_sped_rows}\")\n",
    "print(f\"‚ùå Invalid or unmapped 'CleanedSpEdStudent' values: {invalid_sped_rows}\")\n",
    "print(f\"üìã Frequency breakdown:\")\n",
    "print(sped_counts)\n",
    "\n",
    "# Step 6: Show a sample of invalid rows\n",
    "invalid_sped_df = combined_df_cleaned[\n",
    "    ~combined_df_cleaned['CleanedSpEdStudent'].isin(valid_sped_values)\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Special Education Student' values:\")\n",
    "display(invalid_sped_df[['SchoolName', 'FirstName', 'LastName', 'Special\\n Education\\n Student', 'CleanedSpEdStudent']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fd18d-89c8-4d3b-99e1-d95da693d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backport data from MIEMIS cleaned up records\n",
    "\n",
    "# # Step 1: Compute the match_key in both dataframes (if not already done)\n",
    "# combined_df_cleaned['match_key'] = (\n",
    "#     combined_df_cleaned['FirstName'].str.strip().str.upper() + '|' +\n",
    "#     combined_df_cleaned['LastName'].str.strip().str.upper() + '|' +\n",
    "#     combined_df_cleaned['FixedParsedBirthDate'].astype(str)\n",
    "# )\n",
    "\n",
    "# df_enrolments['match_key'] = (\n",
    "#     df_enrolments['stuGiven'].str.strip().str.upper() + '|' +\n",
    "#     df_enrolments['stuFamilyName'].str.strip().str.upper() + '|' +\n",
    "#     df_enrolments['stuDoB'].astype(str)\n",
    "# )\n",
    "\n",
    "# # Step 2: Build lookup\n",
    "# studentid_lookup = df_enrolments.set_index('match_key')['stuCardID'].to_dict()\n",
    "\n",
    "# # Step 3: Backport with tracking\n",
    "# def backport_student_id(row):\n",
    "#     original = row.get('StudentID')\n",
    "#     if pd.isna(original) or str(original).strip() == '':\n",
    "#         return studentid_lookup.get(row['match_key'], original)\n",
    "#     return original\n",
    "\n",
    "# # Store old values to compare\n",
    "# before = combined_df_cleaned['StudentID'].copy()\n",
    "\n",
    "# # Apply update\n",
    "# combined_df_cleaned['StudentID'] = combined_df_cleaned.apply(backport_student_id, axis=1)\n",
    "\n",
    "# # Step 4: Compare before and after\n",
    "# had_id_before = before.notna() & (before.astype(str).str.strip() != '')\n",
    "# has_id_after = combined_df_cleaned['StudentID'].notna() & (combined_df_cleaned['StudentID'].astype(str).str.strip() != '')\n",
    "# updated_count = has_id_after & ~had_id_before\n",
    "\n",
    "# print(f\"üÜî StudentID backport complete.\")\n",
    "# print(f\"üîπ Had StudentID before: {had_id_before.sum()}\")\n",
    "# print(f\"üîπ Has StudentID now:    {has_id_after.sum()}\")\n",
    "# print(f\"üîÑ Rows updated from df_enrolments: {updated_count.sum()} / {len(combined_df_cleaned)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acefd8f-ac2e-47fa-81cb-4ed03b4b2d28",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Backport a whole bunch of other data from MIEMIS\n",
    "\n",
    "# Step 1: Build match key in both DataFrames\n",
    "combined_df_cleaned['match_key'] = (\n",
    "    combined_df_cleaned['FirstName'].str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['LastName'].str.strip().str.upper() + '|' +\n",
    "    combined_df_cleaned['FixedParsedBirthDate'].astype(str)\n",
    ")\n",
    "\n",
    "df_enrolments['match_key'] = (\n",
    "    df_enrolments['stuGiven'].str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuFamilyName'].str.strip().str.upper() + '|' +\n",
    "    df_enrolments['stuDoB'].astype(str)\n",
    ")\n",
    "\n",
    "# Step 2: Drop duplicates in df_enrolments by match_key (keep last)\n",
    "df_enrolments_deduped = df_enrolments.drop_duplicates(subset='match_key', keep='last')\n",
    "\n",
    "# Step 3: Convert enrolments to dictionary keyed by match_key\n",
    "enrolments_dict = df_enrolments_deduped.set_index('match_key').to_dict(orient='index')\n",
    "\n",
    "# Step 4: Define mappings from df_enrolments ‚Üí combined_df_cleaned\n",
    "field_map = {\n",
    "    'stuCardID': 'StudentID',\n",
    "    'stuEthnicity': 'CleanedEthnicity',\n",
    "    'stueSpEdStr': 'CleanedSpEdStudent',\n",
    "    'SpEdEnv': 'IDEA School Age',\n",
    "    'SpEdDis': 'Disability',\n",
    "    'SpEdEng': 'English Learner',\n",
    "    'stueSpEdHasAccomodationStr': 'Has SBA Accommodation',\n",
    "    'SpEdAcc': 'Type of Accommodation',\n",
    "    'SpEdAss': 'Assessment Type',\n",
    "}\n",
    "\n",
    "# Step 5: Ensure all target columns exist in combined_df_cleaned\n",
    "for tgt_field in field_map.values():\n",
    "    if tgt_field not in combined_df_cleaned.columns:\n",
    "        combined_df_cleaned[tgt_field] = pd.NA\n",
    "\n",
    "# Step 6: Initialize stats and capture pre-update counts\n",
    "backport_stats = {v: 0 for v in field_map.values()}\n",
    "pre_backport_counts = {\n",
    "    v: combined_df_cleaned[v].notna().sum() for v in field_map.values()\n",
    "}\n",
    "\n",
    "# Step 7: Apply backport logic row-by-row\n",
    "def apply_backport(row):\n",
    "    record = enrolments_dict.get(row['match_key'])\n",
    "    if not record:\n",
    "        return row  # No match\n",
    "\n",
    "    for src_field, tgt_field in field_map.items():        \n",
    "        if pd.isna(row[tgt_field]) or str(row[tgt_field]).strip() == '':\n",
    "            if src_field == 'SpEdEnv' and record.get('stueClass', '').upper() in {'GK', 'GPREK'}:\n",
    "                continue\n",
    "            value = record.get(src_field)\n",
    "            if pd.notna(value) and str(value).strip() != '':\n",
    "                row[tgt_field] = value\n",
    "                backport_stats[tgt_field] += 1\n",
    "    return row\n",
    "\n",
    "combined_df_cleaned = combined_df_cleaned.apply(apply_backport, axis=1)\n",
    "\n",
    "# Step 8: Report\n",
    "print(\"üìã Backport Summary:\")\n",
    "for field in field_map.values():\n",
    "    before = pre_backport_counts.get(field, 0)\n",
    "    after = combined_df_cleaned[field].notna().sum()\n",
    "    updated = backport_stats[field]\n",
    "    print(f\"üîπ {field}: {before} ‚Üí {after} (newly updated: {updated})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e4d54-3133-4999-a906-9e40acd70bcc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fill in missing StudentID using predictable format: hash of FIRSTNAME|LASTNAME|DOB\n",
    "import hashlib\n",
    "\n",
    "def generate_student_id(row):\n",
    "    first = str(row['FirstName']).strip().upper()\n",
    "    last = str(row['LastName']).strip().upper()\n",
    "    dob = str(row['FixedParsedBirthDate'])\n",
    "    base_str = f\"{first}|{last}|{dob}\"\n",
    "    hash_obj = hashlib.md5(base_str.encode('utf-8'))\n",
    "    short_hash = hash_obj.hexdigest()[:12]\n",
    "    return f\"SID{short_hash}\"\n",
    "\n",
    "# Identify missing StudentIDs\n",
    "missing_mask = combined_df_cleaned['StudentID'].isna() | (combined_df_cleaned['StudentID'].astype(str).str.strip() == '')\n",
    "\n",
    "# Fill in only the missing values\n",
    "combined_df_cleaned.loc[missing_mask, 'StudentID'] = combined_df_cleaned[missing_mask].apply(generate_student_id, axis=1)\n",
    "\n",
    "print(f\"‚úÖ Filled {missing_mask.sum()} missing StudentID values using predictable hash-based format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15a566-cc37-4bbe-b259-be09bafcdcb6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prepare a draft column mapping\n",
    "\n",
    "# Load the Excel workbook and the Students sheet\n",
    "new_workbook_path = os.path.join(output_directory, empty_census_workbook_filename)\n",
    "wb = load_workbook(filename=new_workbook_path, data_only=True)\n",
    "ws = wb['Students']\n",
    "\n",
    "# Extract column headers from row 3\n",
    "excel_headers = [cell.value for cell in next(ws.iter_rows(min_row=3, max_row=3)) if cell.value]\n",
    "\n",
    "# Lowercase and clean versions of Excel headers for fuzzy matching\n",
    "excel_headers_clean = [str(h).strip().lower().replace(' ', '').replace('_', '') for h in excel_headers]\n",
    "\n",
    "# Clean and prepare DataFrame column headers\n",
    "df_columns = list(combined_df_cleaned.columns)\n",
    "df_columns_clean = [str(c).strip().lower().replace(' ', '').replace('_', '') for c in df_columns]\n",
    "\n",
    "# Attempt to match by index and generate draft mapping\n",
    "mapping = {}\n",
    "for df_col, df_col_clean in zip(df_columns, df_columns_clean):\n",
    "    best_match = None\n",
    "    for excel_col, excel_col_clean in zip(excel_headers, excel_headers_clean):\n",
    "        if df_col_clean == excel_col_clean:\n",
    "            best_match = excel_col\n",
    "            break\n",
    "        if df_col_clean in excel_col_clean or excel_col_clean in df_col_clean:\n",
    "            best_match = excel_col\n",
    "    if best_match:\n",
    "        mapping[df_col] = best_match\n",
    "\n",
    "# Print the mapping string\n",
    "print(\"# Draft column mapping (cleaned_df ‚Üí Excel Students sheet)\\ncolumn_mapping = {\")\n",
    "for k, v in mapping.items():\n",
    "    print(f\"    '{k}': '{v}',\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8a99e-aafb-4301-8a00-0f9ca8056413",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Populate 'First\\nLanguage' based on 'CleanedEthnicity' containing 'Marshallese'\n",
    "\n",
    "mask = combined_df_cleaned['CleanedEthnicity'].astype(str).str.contains('Marshallese', case=False, na=False)\n",
    "combined_df_cleaned.loc[mask, 'First\\n Language'] = 'Marshallese'\n",
    "\n",
    "print(f\"‚úÖ Set 'First\\\\nLanguage' to 'Marshallese' for {mask.sum()} students based on ethnicity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35294be-a228-48cc-8dbf-0b4447121d39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import xlwings as xw\n",
    "import pandas as pd\n",
    "\n",
    "# üîÅ Always start with a clean copy of the workbook\n",
    "new_workbook_path = os.path.join(output_directory, empty_census_workbook_filename)\n",
    "clean_workbook_path = os.path.join(output_directory, clean_census_workbook_filename)\n",
    "\n",
    "# üßπ Delete and copy clean workbook template (controlled by flag)\n",
    "if delete_census_workbook_filename:\n",
    "    if os.path.exists(new_workbook_path):\n",
    "        try:\n",
    "            os.remove(new_workbook_path)\n",
    "            print(f\"üóëÔ∏è Removed previous workbook: {new_workbook_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to delete existing workbook: {new_workbook_path}\\n{e}\")\n",
    "\n",
    "    try:\n",
    "        shutil.copyfile(clean_workbook_path, new_workbook_path)\n",
    "        print(f\"üìÑ Copied clean template to: {new_workbook_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to copy clean workbook template.\\n{e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping deletion and copy of census workbook (using existing workbook).\")\n",
    "\n",
    "# Mapping from cleaned dataframe to census workbook dataframe columns (edit as needed)\n",
    "column_mapping = {\n",
    "    'CleanedSchoolName': 'School Name',\n",
    "    'SchoolYear': 'SchoolYear',\n",
    "    'StudentID': 'National Student ID',\n",
    "    'FirstName': 'First Name',\n",
    "    'LastName': 'Last Name',\n",
    "    'Gender': 'Gender',\n",
    "    'Assessment Type': 'Assessment Type',\n",
    "    'Exiting': 'Exiting',\n",
    "    'First\\n Language': 'Language',\n",
    "    'FixedParsedBirthDate': 'Date of Birth',\n",
    "    'DoB Estimate': 'DoB Estimate',\n",
    "    'InferredAttendedECE': 'Attended ECE',\n",
    "    'Grade': 'Grade Level',\n",
    "    'CleanedFrom': 'From',\n",
    "    'InferredTransferredFromWhichSchool': 'Transferred From which school',\n",
    "    #'CleanedTransferredInDate': 'Transfer In Date',\n",
    "    'CleanedEthnicity': 'Ethnicity',\n",
    "    'CleanedCitizenship': 'Citizenship',\n",
    "    'CleanedSpEdStudent': 'SpEd Student',\n",
    "    # 'IDEA ECE': 'IDEA ECE',\n",
    "    'IDEA School Age': 'IDEA School Age',\n",
    "    'Disability': 'Disability',\n",
    "    'English Learner': 'English Learner',\n",
    "    'Has IEP': 'Has IEP',\n",
    "    'Has SBA Accommodation': 'Has SBA Accommodation',\n",
    "    'Type of Accommodation': 'Type of Accommodation',\n",
    "    # 'Assessment Type'\n",
    "    # 'Exiting'\n",
    "    # 'Exiting Date'\n",
    "    # 'Days Absent'\n",
    "    # 'Completed?'\n",
    "    # 'Outcome'\n",
    "    # 'Dropout Reason'\n",
    "    # 'Expulsion Reason'\n",
    "    # 'Transferred To Which School'\n",
    "    # 'Post-secondary study'\n",
    "    # 'Bullied'\n",
    "}\n",
    "    \n",
    "# Open the workbook with xlwings (preserves formatting, macros, formulas)\n",
    "try:\n",
    "    app = xw.App(visible=False)\n",
    "    wb = app.books.open(new_workbook_path)\n",
    "    \n",
    "    # Get the correct worksheet\n",
    "    sheet_name = 'Students'\n",
    "    ws = wb.sheets[sheet_name]\n",
    "    \n",
    "    # ‚úÖ Unprotect the correct sheet\n",
    "    ws.api.Unprotect()  # Add password if needed\n",
    "    \n",
    "    # Prepare DataFrame\n",
    "    df_to_insert = combined_df_cleaned[list(column_mapping.keys())].copy()\n",
    "    df_to_insert.rename(columns=column_mapping, inplace=True)\n",
    "    df_to_insert = df_to_insert.astype(object).where(pd.notna(df_to_insert), None)\n",
    "    \n",
    "    # Read Excel headers from row 3\n",
    "    header_row = 3\n",
    "    excel_headers = ws.range((header_row, 1)).expand('right').value\n",
    "    header_indices = {\n",
    "        header: idx + 1 for idx, header in enumerate(excel_headers) if header in df_to_insert.columns\n",
    "    }\n",
    "    \n",
    "    # Test on small subset\n",
    "    #df_to_insert = df_to_insert[:1000].copy()\n",
    "    \n",
    "    # # Write row by row approach\n",
    "    # # Start inserting data from row 4\n",
    "    # start_row = header_row + 1\n",
    "    # for i, (_, row) in enumerate(df_to_insert.iterrows(), start=start_row):\n",
    "    #     #print(f\"Writing row {i}\")\n",
    "    #     #if (i - start_row + 1) % 50 == 0:\n",
    "    #     #    print(f\"Writing row {i}\")\n",
    "    #     for col_name, value in row.items():\n",
    "    #         col_idx = header_indices.get(col_name)\n",
    "    #         if col_idx:\n",
    "    #             ws.cells(i, col_idx).value = value\n",
    "    \n",
    "    # Write using diagnostic vectorized approach\n",
    "    start_row = header_row + 1\n",
    "    num_rows = len(df_to_insert)\n",
    "    \n",
    "    invalid_columns = []\n",
    "    \n",
    "    for col_name, col_idx in header_indices.items():\n",
    "        try:\n",
    "            # Try writing one column vector at a time\n",
    "            col_values = df_to_insert[col_name].tolist()\n",
    "            ws.range((start_row, col_idx), (start_row + num_rows - 1, col_idx)).value = [[v] for v in col_values]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in column: {col_name} (Excel column {col_idx})\")\n",
    "            print(e)\n",
    "            invalid_columns.append(col_name)\n",
    "\n",
    "    if len(invalid_columns) > 0:\n",
    "        print(f\"\\nüö® Columns that failed to write: {invalid_columns}\")\n",
    "\n",
    "    \n",
    "    # Optionally re-protect the sheet\n",
    "    # ws.api.Protect()\n",
    "\n",
    "    # Save and close\n",
    "    wb.save()\n",
    "finally:\n",
    "    wb.close()\n",
    "    app.quit()\n",
    "\n",
    "print(\"‚úÖ Data successfully injected into Excel workbook without touching formulas or formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1cf17-e7d7-41b1-bb06-9accdecda925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_insert[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d61c7-e29c-4bee-aa3b-ff003e5efc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_insert.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569f801-61f3-4ff7-a2b9-be26673fdf0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Look at duplicates\n",
    "\n",
    "# Normalize the DataFrame\n",
    "df = df_to_insert.copy()\n",
    "\n",
    "# Clean up key fields\n",
    "df['National Student ID'] = df['National Student ID'].astype(str).str.strip().str.upper()\n",
    "df['First Name'] = df['First Name'].astype(str).str.strip().str.upper()\n",
    "df['Last Name'] = df['Last Name'].astype(str).str.strip().str.upper()\n",
    "df['Date of Birth'] = pd.to_datetime(df['Date of Birth'], errors='coerce')\n",
    "\n",
    "# ‚úÖ 1. Duplicates by National Student ID + First + Last + DOB\n",
    "dupe_keys_1 = ['National Student ID', 'First Name', 'Last Name', 'Date of Birth']\n",
    "df_to_insert_dupes1 = df[df.duplicated(dupe_keys_1, keep=False)].sort_values(by=dupe_keys_1)\n",
    "\n",
    "# ‚úÖ 2. Duplicates by First + Last + DOB only (ignoring NSID)\n",
    "dupe_keys_2 = ['First Name', 'Last Name', 'Date of Birth']\n",
    "df_to_insert_dupes2 = df[df.duplicated(dupe_keys_2, keep=False)].sort_values(by=dupe_keys_2)\n",
    "\n",
    "# ‚úÖ Summary\n",
    "print(f\"üßæ Duplicates by National Student ID + Name + DOB: {df_to_insert_dupes1.shape[0]} rows in {df_to_insert_dupes1[dupe_keys_1].drop_duplicates().shape[0]} groups.\")\n",
    "print(f\"üßæ Duplicates by Name + DOB only: {df_to_insert_dupes2.shape[0]} rows in {df_to_insert_dupes2[dupe_keys_2].drop_duplicates().shape[0]} groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bdfd9-1986-4be0-95b4-b50d7d87e3ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the output file name\n",
    "output_duplicates_path = new_workbook_path.replace(\".xlsm\", \"-duplicate-students.xlsx\")\n",
    "\n",
    "# Save both DataFrames into a new Excel workbook\n",
    "with pd.ExcelWriter(output_duplicates_path, engine='xlsxwriter') as writer:\n",
    "    df_to_insert_dupes1.to_excel(writer, sheet_name='Student Duplicates 1', index=False)\n",
    "    df_to_insert_dupes2.to_excel(writer, sheet_name='Student Duplicates 2', index=False)\n",
    "\n",
    "print(f\"‚úÖ Duplicate records written to: {output_duplicates_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4ef50-f200-4308-9995-07a02afe8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
