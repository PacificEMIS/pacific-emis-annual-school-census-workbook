{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed61114-fdcf-4ac4-a61c-912af5326355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "import shutil\n",
    "\n",
    "def load_config(config_path=\"config.json\"):\n",
    "    \"\"\"Load configuration from a JSON file.\"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config['output_directory'], config['source_workbook_filename'], config['empty_census_workbook_filename'], config['clean_census_workbook_filename'], config['delete_census_workbook_filename']\n",
    "    \n",
    "\n",
    "# Test loading configuration\n",
    "output_directory, source_workbook_filename, empty_census_workbook_filename, clean_census_workbook_filename, delete_census_workbook_filename = load_config()\n",
    "print(\"Configuration loaded successfully.\")\n",
    "\n",
    "# Import some lookups\n",
    "%store -r core_lookups student_lookups censusworkbook_lookups\n",
    "%store -r all_teachers all_schools \n",
    "%store -r df_teacher_recent_survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02e44e-ae75-46a1-9233-91698ade8d50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load workbook\n",
    "\n",
    "# Combine the directory and filename\n",
    "workbook_path = os.path.join(output_directory, source_workbook_filename)\n",
    "\n",
    "# Load the workbook (but not yet any sheet into memory)\n",
    "xls = pd.ExcelFile(workbook_path)\n",
    "\n",
    "# See the available sheet names\n",
    "print(\"Sheets found:\", xls.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4d65b-e6d6-468c-b40d-c347f2bd39bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Get Staff data from \"Staff Roster\" sheet\n",
    "\n",
    "# Build the full path\n",
    "workbook_path = os.path.join(output_directory, source_workbook_filename)\n",
    "\n",
    "# Load the Excel file\n",
    "xls = pd.ExcelFile(workbook_path)\n",
    "\n",
    "# Specify the staff sheet\n",
    "staff_sheet = \"Staff Roster\"\n",
    "\n",
    "# Load the sheet\n",
    "df_staff_raw = xls.parse(staff_sheet)\n",
    "\n",
    "# Display basic preview\n",
    "display(df_staff_raw.head(3))\n",
    "\n",
    "# Summary info\n",
    "total_rows = df_staff_raw.shape[0]\n",
    "column_list = list(df_staff_raw.columns)\n",
    "\n",
    "print(f\"‚úÖ Successfully loaded '{staff_sheet}' with {total_rows} rows.\")\n",
    "print(f\"üßæ Columns in df_staff_raw ({len(column_list)}):\")\n",
    "print(column_list)\n",
    "\n",
    "# Summary per School (if present)\n",
    "if 'School' in df_staff_raw.columns:\n",
    "    print(\"\\nüìä Row counts per School:\")\n",
    "    print(df_staff_raw['School'].value_counts(dropna=False).sort_index())\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è 'School' column not found in the staff roster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af6448-4362-4622-af0e-7336deeb1f2a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Filter staff records for the current school year 'SY24-25'\n",
    "\n",
    "# Define the target year code\n",
    "target_staff_year = \"SY24-25\"\n",
    "\n",
    "# Filter records\n",
    "df_staff_filtered = df_staff_raw[df_staff_raw['Year'] == target_staff_year].copy()\n",
    "\n",
    "# Display a preview\n",
    "display(df_staff_filtered.head(3))\n",
    "\n",
    "# Summary info\n",
    "filtered_count = df_staff_filtered.shape[0]\n",
    "print(f\"‚úÖ Filtered staff records to {filtered_count} row(s) for Year == '{target_staff_year}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b40fa8-d83a-4bc5-b335-b7bcf53c8d99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Remove junk rows from staff data (missing key info)\n",
    "\n",
    "# Define mask: True for rows where all key fields are either NaN or blank\n",
    "junk_mask = (\n",
    "    df_staff_filtered['First Name'].isna() | df_staff_filtered['First Name'].astype(str).str.strip().eq('')\n",
    ") & (\n",
    "    df_staff_filtered['Last Name'].isna() | df_staff_filtered['Last Name'].astype(str).str.strip().eq('')\n",
    ") & (\n",
    "    df_staff_filtered['School'].isna() | df_staff_filtered['School'].astype(str).str.strip().eq('')\n",
    ")\n",
    "\n",
    "# Count and drop junk rows\n",
    "junk_count = junk_mask.sum()\n",
    "df_staff_filtered = df_staff_filtered[~junk_mask].copy()\n",
    "\n",
    "# Display preview\n",
    "display(df_staff_filtered.head(3))\n",
    "\n",
    "# Summary\n",
    "print(f\"üßπ Removed {junk_count} junk row(s) with missing First Name, Last Name, and School.\")\n",
    "print(f\"‚úÖ Remaining valid staff records: {df_staff_filtered.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b99339-2417-4aa8-a060-de4f051e4da4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean 'Year' column into 'CleanedYear' with full 4-digit format\n",
    "\n",
    "import re\n",
    "\n",
    "def standardize_year(year_value):\n",
    "    if isinstance(year_value, str):\n",
    "        match = re.match(r'^SY?(\\d{2})[-‚Äì](\\d{2})$', year_value.strip())\n",
    "        if match:\n",
    "            start, end = match.groups()\n",
    "            start_full = int(start)\n",
    "            end_full = int(end)\n",
    "            # Assume school years are within the same century (20xx)\n",
    "            if start_full < 50:  # e.g., 24 -> 2024\n",
    "                start_full += 2000\n",
    "            else:  # unlikely but handles 90s as 199x\n",
    "                start_full += 1900\n",
    "            if end_full < 50:\n",
    "                end_full += 2000\n",
    "            else:\n",
    "                end_full += 1900\n",
    "            return f\"SY{start_full}-{end_full}\"\n",
    "    return year_value  # fallback to original if no match\n",
    "\n",
    "# Apply the cleaning\n",
    "df_staff_filtered['CleanedYear'] = df_staff_filtered['Year'].apply(standardize_year)\n",
    "\n",
    "# Show unique cleaned values\n",
    "unique_cleaned_years = df_staff_filtered['CleanedYear'].dropna().unique()\n",
    "print(\"üìÖ Unique cleaned 'CleanedYear' values:\")\n",
    "for val in sorted(unique_cleaned_years):\n",
    "    print(f\" - {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1698e-86cb-49e1-bb07-658a23537808",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean 'School' into 'CleanedSchool' using lookup with manual mapping support\n",
    "\n",
    "# Step 1: Extract valid school names from lookup\n",
    "valid_school_names = {entry['N'] for entry in censusworkbook_lookups['schoolCodes']}\n",
    "\n",
    "# Step 2: Create initial CleanedSchool where value is valid\n",
    "df_staff_filtered['CleanedSchool'] = df_staff_filtered['School'].where(\n",
    "    df_staff_filtered['School'].isin(valid_school_names), pd.NA\n",
    ")\n",
    "\n",
    "# Step 3: Identify unmatched values to build mapping\n",
    "unmatched_schools = df_staff_filtered.loc[df_staff_filtered['CleanedSchool'].isna(), 'School'].dropna().unique()\n",
    "\n",
    "print(\"üìã Unmatched 'School' values (please review and build mapping):\")\n",
    "for val in sorted(unmatched_schools):\n",
    "    print(f\" - '{val}'\")\n",
    "\n",
    "# Step 4: Create manual mapping dictionary (üîÅ you update this)\n",
    "manual_school_mapping = {\n",
    "    'Jabenoden ES-Jaluit': 'Jabnodren Elementary School',\n",
    "    # Example:\n",
    "    # 'Ajeltake Elem School': 'Ajeltake Elementary School',\n",
    "}\n",
    "\n",
    "# Step 5: Apply manual mapping to fill in CleanedSchool\n",
    "df_staff_filtered.loc[\n",
    "    df_staff_filtered['School'].isin(manual_school_mapping.keys()),\n",
    "    'CleanedSchool'\n",
    "] = df_staff_filtered['School'].map(manual_school_mapping)\n",
    "\n",
    "# Step 6: Final validation and summary\n",
    "final_valid_count = df_staff_filtered['CleanedSchool'].notna().sum()\n",
    "final_invalid_count = df_staff_filtered.shape[0] - final_valid_count\n",
    "still_unmatched = df_staff_filtered.loc[df_staff_filtered['CleanedSchool'].isna(), 'School'].dropna().unique()\n",
    "\n",
    "print(f\"\\nüèÅ Final CleanedSchool summary:\")\n",
    "print(f\"‚úÖ Matched or corrected: {final_valid_count}\")\n",
    "print(f\"‚ùå Still unmatched: {final_invalid_count}\")\n",
    "if still_unmatched.size > 0:\n",
    "    print(\"‚ö†Ô∏è Still unmatched values after manual mapping:\")\n",
    "    for val in sorted(still_unmatched):\n",
    "        print(f\" - '{val}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014330ae-5f7e-47c0-8913-b614d25a1d31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Build CleanedSchNo based on CleanedSchool using lookup\n",
    "\n",
    "# Step 1: Build dictionary from CleanedSchool to School Code\n",
    "school_name_to_code = {entry['N']: entry['C'] for entry in censusworkbook_lookups['schoolCodes']}\n",
    "\n",
    "# Step 2: Map CleanedSchool to CleanedSchNo\n",
    "df_staff_filtered['CleanedSchNo'] = df_staff_filtered['CleanedSchool'].map(school_name_to_code)\n",
    "\n",
    "# Step 3: Validation summary\n",
    "valid_schno_count = df_staff_filtered['CleanedSchNo'].notna().sum()\n",
    "invalid_schno_rows = df_staff_filtered['CleanedSchool'].notna() & df_staff_filtered['CleanedSchNo'].isna()\n",
    "unmatched_cleaned_schools = df_staff_filtered.loc[invalid_schno_rows, 'CleanedSchool'].unique()\n",
    "\n",
    "print(f\"‚úÖ Successfully populated CleanedSchNo for {valid_schno_count} rows.\")\n",
    "if unmatched_cleaned_schools.size > 0:\n",
    "    print(f\"‚ö†Ô∏è Could not find CleanedSchNo for the following CleanedSchool values:\")\n",
    "    for name in sorted(unmatched_cleaned_schools):\n",
    "        print(f\" - '{name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf415d-d64e-47db-9ad3-1c5f0d62068a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean and Validate Gender in Staff Data\n",
    "\n",
    "# Step 1: Define valid values\n",
    "valid_genders = {'Male', 'Female'}\n",
    "\n",
    "# Step 2: Standardize raw Gender values\n",
    "df_staff_filtered['Gender'] = df_staff_filtered['Gender'].astype(str).str.strip()\n",
    "\n",
    "# Step 3: Flag initial invalids\n",
    "df_staff_filtered['Invalid_Gender'] = ~df_staff_filtered['Gender'].isin(valid_genders)\n",
    "initial_invalids = df_staff_filtered[df_staff_filtered['Invalid_Gender']]\n",
    "print(f\"‚ö†Ô∏è Found {initial_invalids.shape[0]} row(s) with invalid Gender before correction.\")\n",
    "\n",
    "# Step 4: Apply corrections\n",
    "gender_corrections = {\n",
    "    'M': 'Male',\n",
    "    'F': 'Female',\n",
    "    'm': 'Male',\n",
    "    'f': 'Female',\n",
    "    'male': 'Male',\n",
    "    'female': 'Female',\n",
    "    'Femaler': 'Female',\n",
    "    'Feamale': 'Female',\n",
    "    'MALE': 'Male',\n",
    "    'FEMALE': 'Female',\n",
    "    'John': 'Male',\n",
    "}\n",
    "df_staff_filtered['CleanedGender'] = df_staff_filtered['Gender'].replace(gender_corrections)\n",
    "\n",
    "# Step 5: Re-flag invalids after correction\n",
    "df_staff_filtered['Invalid_Gender'] = ~df_staff_filtered['CleanedGender'].isin(valid_genders)\n",
    "\n",
    "# Step 6: Summary\n",
    "remaining_invalids = df_staff_filtered['Invalid_Gender'].sum()\n",
    "print(f\"‚úÖ Gender correction complete. Remaining invalid rows: {remaining_invalids}\")\n",
    "\n",
    "# Optional: Preview a few remaining invalids\n",
    "if remaining_invalids:\n",
    "    display(df_staff_filtered[df_staff_filtered['Invalid_Gender']][['School', 'CleanedGender', 'SourceSheet']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a46d34-026c-48c1-9121-bcbf06868ffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Clean and Parse 'Date of Birth' into 'CleanedDateofBirth'\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "def try_parse_dob(value):\n",
    "    \"\"\"Attempt to parse a date of birth value with fallback logic.\"\"\"\n",
    "    if pd.isna(value) or str(value).strip() == '':\n",
    "        return None\n",
    "\n",
    "    # Handle Excel serial numbers\n",
    "    if isinstance(value, (int, float)) and value > 59:\n",
    "        try:\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(int(value), unit='D')\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Try parsing string formats\n",
    "    str_val = str(value).strip()\n",
    "    for fmt in (\"%m/%d/%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%B %d, %Y\", \"%d-%b-%y\"):\n",
    "        try:\n",
    "            return datetime.datetime.strptime(str_val, fmt).date()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Try auto parser\n",
    "    try:\n",
    "        return parser.parse(str_val, dayfirst=False).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Step 1: Apply DOB parser\n",
    "df_staff_filtered['CleanedDateofBirth'] = df_staff_filtered['Date of Birth'].apply(try_parse_dob)\n",
    "\n",
    "# Step 2: Flag rows that failed parsing\n",
    "df_staff_filtered['Invalid_DOB'] = df_staff_filtered['CleanedDateofBirth'].isna() & df_staff_filtered['Date of Birth'].notna()\n",
    "\n",
    "# Step 3: Summary\n",
    "total_invalid = df_staff_filtered['Invalid_DOB'].sum()\n",
    "print(f\"‚úÖ Completed Date of Birth cleaning.\")\n",
    "print(f\"‚ö†Ô∏è Found {total_invalid} row(s) with unparseable Date of Birth.\")\n",
    "\n",
    "# Optional: Show a few problematic rows\n",
    "if total_invalid:\n",
    "    display(df_staff_filtered[df_staff_filtered['Invalid_DOB']][['School', 'First Name', 'Last Name', 'Date of Birth']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d3627-9e8e-4a35-b070-1152d84c9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Build a lookup dictionary from existing teachers based on (First, Last) name\n",
    "teacher_dob_lookup = {\n",
    "    (t['tGiven'].strip().lower(), t['tSurname'].strip().lower()): t['tDOB']\n",
    "    for t in all_teachers\n",
    "    if t['tGiven'] and t['tSurname'] and t['tDOB'] not in (None, '', '1900-01-01T00:00:00')  # ignore placeholders\n",
    "}\n",
    "\n",
    "# Step 2: Initialize counters\n",
    "forward_ported_missing = 0\n",
    "forward_ported_unparsed = 0\n",
    "\n",
    "# Step 3: Go through rows and attempt forward porting\n",
    "for idx, row in df_staff_filtered.iterrows():\n",
    "    dob = row.get('CleanedDateofBirth')\n",
    "    first = str(row.get('First Name', '')).strip().lower()\n",
    "    last = str(row.get('Last Name', '')).strip().lower()\n",
    "\n",
    "    if pd.isna(dob) and (first, last) in teacher_dob_lookup:\n",
    "        dob_str = teacher_dob_lookup[(first, last)]\n",
    "        try:\n",
    "            parsed_dob = parser.parse(dob_str).date()\n",
    "            df_staff_filtered.at[idx, 'CleanedDateofBirth'] = parsed_dob\n",
    "\n",
    "            # Classify forward-port type\n",
    "            original_raw = str(row.get('Date of Birth', '')).strip()\n",
    "            if original_raw == '' or pd.isna(row['Date of Birth']):\n",
    "                forward_ported_missing += 1\n",
    "            else:\n",
    "                forward_ported_unparsed += 1\n",
    "\n",
    "        except Exception:\n",
    "            continue  # if even the original tDOB is unparseable, skip\n",
    "\n",
    "# Step 4: Print summary\n",
    "print(f\"üì¶ Forward-port complete.\")\n",
    "print(f\"‚úÖ {forward_ported_missing} record(s) filled from teachers where DOB was missing.\")\n",
    "print(f\"‚ôªÔ∏è  {forward_ported_unparsed} record(s) filled from teachers where DOB was unparsed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc91da-44eb-4d3d-9c23-b73b382ff4dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 1: Load valid citizenship names from lookup\n",
    "valid_citizenships = {entry['N'] for entry in censusworkbook_lookups['nationalities']}\n",
    "valid_citizenships_lower = {v.lower(): v for v in valid_citizenships}\n",
    "\n",
    "# Step 2: Extract unique raw citizenships\n",
    "raw_citizenships = df_staff_filtered['Citizenship'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Build draft mapping\n",
    "draft_mapping = {}\n",
    "\n",
    "for raw_value in raw_citizenships:\n",
    "    val = raw_value.strip()\n",
    "    val_lower = val.lower()\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val in valid_citizenships:\n",
    "        draft_mapping[val] = val\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_citizenships_lower:\n",
    "        draft_mapping[val] = valid_citizenships_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val, valid_citizenships, n=1, cutoff=0.8)\n",
    "    if close:\n",
    "        draft_mapping[val] = close[0]\n",
    "    else:\n",
    "        draft_mapping[val] = None  # Needs manual review\n",
    "\n",
    "# Step 4: Apply mapping\n",
    "df_staff_filtered['CleanedCitizenship'] = (\n",
    "    df_staff_filtered['Citizenship'].astype(str).str.strip().map(draft_mapping)\n",
    ")\n",
    "\n",
    "# Step 5: Summary\n",
    "print(\"üìã Draft automatic mapping:\")\n",
    "for k, v in sorted(draft_mapping.items()):\n",
    "    print(f\"'{k}': '{v}'\")\n",
    "\n",
    "valid_count = df_staff_filtered['CleanedCitizenship'].isin(valid_citizenships).sum()\n",
    "total_rows = df_staff_filtered.shape[0]\n",
    "invalid_count = total_rows - valid_count\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid citizenships: {valid_count}\")\n",
    "print(f\"‚ùå Remaining unmapped or invalid citizenships: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba695083-7e66-427c-a397-8a74b41f9fa6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 1: Load valid citizenships from lookup\n",
    "valid_citizenships = {entry['N'] for entry in core_lookups['nationalities']}\n",
    "valid_citizenships_lower = {v.lower(): v for v in valid_citizenships}\n",
    "\n",
    "# Step 2: Start with manual mapping\n",
    "citizenship_mapping = {\n",
    "    'Brazil': 'Brazil',\n",
    "    'Cameroon': 'Cameroon',\n",
    "    'Canada': 'Canada',\n",
    "    'Chuuk': 'FSM',\n",
    "    'Chuuk Islands': 'FSM',\n",
    "    'Denmark': 'Denmark',\n",
    "    'FIJ': 'Fiji',\n",
    "    'FIJI': 'Fiji',\n",
    "    'FSM': 'FSM',\n",
    "    'Fiji': 'Fiji',\n",
    "    'Fijian': 'Fiji',\n",
    "    'JAPAN': 'Japan',\n",
    "    'KIRIBATI': 'Kiribati',\n",
    "    'Kirbati': 'Kiribati',\n",
    "    'Kiribati': 'Kiribati',\n",
    "    'Kiribati Isalnds': 'Kiribati',\n",
    "    'Kiribati Islands': 'Kiribati',\n",
    "    'Marshall Isalnds': 'Marshall Islands',\n",
    "    'Marshall Islands': 'Marshall Islands',\n",
    "    'NIGERIA': 'Nigeria',\n",
    "    'PHI': 'Philippines',\n",
    "    'PI': 'Philippines',\n",
    "    'PNG': 'Papua New Guinea',\n",
    "    'POH': 'FSM',\n",
    "    'Pakistan': 'Pakistan',\n",
    "    'Papua New Guinea': 'Papua New Guinea',\n",
    "    'Philippines': 'Philippines',\n",
    "    'Phillipines': 'Philippines',\n",
    "    'Pohnpei': 'FSM',\n",
    "    'RMI': 'Marshall Islands',\n",
    "    'ROP': 'Palau',\n",
    "    'ROV': 'Vanuatu',\n",
    "    'SOL': 'Solomon Islands',\n",
    "    'SOLOMON': 'Solomon Islands',\n",
    "    'Solmon Islands': 'Solomon Islands',\n",
    "    'Solomon Islands': 'Solomon Islands',\n",
    "    'South Africa': 'South Africa',\n",
    "    'TUV': 'Tuvalu',\n",
    "    'Taiwan': 'Taiwan',\n",
    "    'Tuvalu': 'Tuvalu',\n",
    "    'UK': 'United Kingdom',\n",
    "    'US': 'USA',\n",
    "    'USA': 'USA',\n",
    "    'Zimbabwe': 'Zimbabwe',\n",
    "}\n",
    "\n",
    "# Step 3: Get all raw values from the data\n",
    "raw_citizenships = df_staff_filtered['Citizenship'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 4: Try to auto-map values not already manually mapped\n",
    "for val in raw_citizenships:\n",
    "    if val in citizenship_mapping:\n",
    "        continue  # already mapped manually\n",
    "\n",
    "    val_stripped = val.strip()\n",
    "    val_lower = val_stripped.lower()\n",
    "\n",
    "    # Case-insensitive match\n",
    "    if val_lower in valid_citizenships_lower:\n",
    "        citizenship_mapping[val] = valid_citizenships_lower[val_lower]\n",
    "    else:\n",
    "        # Fuzzy match\n",
    "        close = get_close_matches(val_stripped, valid_citizenships, n=1, cutoff=0.8)\n",
    "        if close:\n",
    "            citizenship_mapping[val] = close[0]\n",
    "        else:\n",
    "            citizenship_mapping[val] = None  # needs manual review\n",
    "\n",
    "# Step 5: Apply mapping\n",
    "df_staff_filtered['CleanedCitizenship'] = (\n",
    "    df_staff_filtered['Citizenship'].astype(str).str.strip().map(citizenship_mapping)\n",
    ")\n",
    "\n",
    "# Step 6: Summary\n",
    "print(\"\\nüìã Final citizenship mapping:\")\n",
    "for k, v in sorted(citizenship_mapping.items()):\n",
    "    print(f\"'{k}': '{v}'\")\n",
    "\n",
    "valid_count = df_staff_filtered['CleanedCitizenship'].isin(valid_citizenships).sum()\n",
    "invalid_count = df_staff_filtered.shape[0] - valid_count\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid citizenships: {valid_count}\")\n",
    "print(f\"‚ùå Remaining unmapped or invalid: {invalid_count}\")\n",
    "\n",
    "# Optional: Show sample of invalids\n",
    "invalid_df = df_staff_filtered[~df_staff_filtered['CleanedCitizenship'].isin(valid_citizenships)]\n",
    "display(invalid_df[['School', 'First Name', 'Last Name', 'Citizenship', 'CleanedCitizenship']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1ef58-9705-44eb-a672-164f945aa137",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 1: Load valid ethnicity names from lookup\n",
    "valid_ethnicities = {entry['N'] for entry in censusworkbook_lookups['ethnicities']}\n",
    "valid_ethnicities_lower = {v.lower(): v for v in valid_ethnicities}\n",
    "\n",
    "# Step 2: Extract unique raw ethnicities\n",
    "raw_ethnicities = df_staff_filtered['Ethnicity'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Start with manual mapping\n",
    "ethnicity_mapping = {\n",
    "    'African': 'Other African',\n",
    "    'American': np.nan,\n",
    "    'Asian': 'Other Asian',\n",
    "    'Brazilian': 'Brazilian',\n",
    "    'British': 'Caucasian',\n",
    "    'Cameroonian': 'Cameroonian',\n",
    "    'Canadian': np.nan,\n",
    "    'Caucasian': 'Caucasian',\n",
    "    'Chuukese': 'Chuukese',\n",
    "    'FIjian': 'Fijian',\n",
    "    'Fijian': 'Fijian',\n",
    "    'Filipino': 'Filipino',\n",
    "    'Filipinos': 'Filipino',\n",
    "    'Fillipino': 'Filipino',\n",
    "    'I Kiribati': 'Kiribatese',\n",
    "    'Japanese': 'Japanese',\n",
    "    'Kiribatese': 'Kiribatese',\n",
    "    'Marshallese': 'Marshallese',\n",
    "    'Other': 'Other',\n",
    "    'Pacific Islander': 'Other Pacific Islander',\n",
    "    'Pakistanis': 'Pakistani',\n",
    "    'Palauan': 'Palauan',\n",
    "    'Papua New Guinea': 'Papua New Guinean',\n",
    "    'Pohnpeian': 'Pohnpeian',\n",
    "    'Shona': np.nan,\n",
    "    'Solomon': 'Solomon Islander',\n",
    "    'Taiwanese': 'Taiwanese',\n",
    "    'Tuvaluan': 'Tuvaluan',\n",
    "    'Vanutuan': 'Ni Vanuatu',\n",
    "}\n",
    "\n",
    "# Step 4: Add auto-mapping ONLY for values not already manually mapped\n",
    "for raw_value in raw_ethnicities:\n",
    "    val = raw_value.strip()\n",
    "    val_lower = val.lower()\n",
    "\n",
    "    if val in ethnicity_mapping:\n",
    "        continue  # already mapped\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val in valid_ethnicities:\n",
    "        ethnicity_mapping[val] = val\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_ethnicities_lower:\n",
    "        ethnicity_mapping[val] = valid_ethnicities_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val, valid_ethnicities, n=1, cutoff=0.8)\n",
    "    if close:\n",
    "        ethnicity_mapping[val] = close[0]\n",
    "    else:\n",
    "        ethnicity_mapping[val] = None  # Needs manual review\n",
    "\n",
    "# Step 5: Apply mapping\n",
    "df_staff_filtered['CleanedEthnicity'] = (\n",
    "    df_staff_filtered['Ethnicity'].astype(str).str.strip().map(ethnicity_mapping)\n",
    ")\n",
    "\n",
    "# Step 6: Summary\n",
    "print(\"üìã Final ethnicity mapping:\")\n",
    "for k, v in sorted(ethnicity_mapping.items()):\n",
    "    print(f\"'{k}': '{v}'\")\n",
    "\n",
    "valid_count = df_staff_filtered['CleanedEthnicity'].isin(valid_ethnicities).sum()\n",
    "total_rows = df_staff_filtered.shape[0]\n",
    "invalid_count = total_rows - valid_count\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid ethnicities: {valid_count}\")\n",
    "print(f\"‚ùå Remaining unmapped or invalid ethnicities: {invalid_count}\")\n",
    "\n",
    "# Step 7: Frequency breakdown\n",
    "ethnicity_counts = df_staff_filtered['CleanedEthnicity'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedEthnicity':\")\n",
    "print(ethnicity_counts)\n",
    "\n",
    "# Step 8: Sample invalid rows\n",
    "invalid_ethnicity_df = df_staff_filtered[\n",
    "    ~df_staff_filtered['CleanedEthnicity'].isin(valid_ethnicities)\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Ethnicity' values:\")\n",
    "display(invalid_ethnicity_df[['School', 'First Name', 'Last Name', 'Ethnicity', 'CleanedEthnicity']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68402af4-e518-499b-82d5-354045597f01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 1: Normalize and prepare keys\n",
    "df_staff_filtered['First Name'] = df_staff_filtered['First Name'].astype(str).str.strip().str.lower()\n",
    "df_staff_filtered['Last Name'] = df_staff_filtered['Last Name'].astype(str).str.strip().str.lower()\n",
    "df_staff_filtered['CleanedDateofBirth'] = pd.to_datetime(df_staff_filtered['CleanedDateofBirth'], errors='coerce')\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_teachers_df = pd.DataFrame(all_teachers)\n",
    "\n",
    "all_teachers_df['tGiven'] = all_teachers_df['tGiven'].astype(str).str.strip().str.lower()\n",
    "all_teachers_df['tSurname'] = all_teachers_df['tSurname'].astype(str).str.strip().str.lower()\n",
    "all_teachers_df['tDOB'] = pd.to_datetime(all_teachers_df['tDOB'], errors='coerce')\n",
    "\n",
    "# Step 2: Join keys\n",
    "df_staff_filtered['strict_key'] = (\n",
    "    df_staff_filtered['First Name'] + '|' +\n",
    "    df_staff_filtered['Last Name'] + '|' +\n",
    "    df_staff_filtered['CleanedDateofBirth'].astype(str)\n",
    ")\n",
    "\n",
    "df_staff_filtered['loose_key'] = (\n",
    "    df_staff_filtered['First Name'] + '|' +\n",
    "    df_staff_filtered['Last Name']\n",
    ")\n",
    "\n",
    "all_teachers_df['strict_key'] = (\n",
    "    all_teachers_df['tGiven'] + '|' +\n",
    "    all_teachers_df['tSurname'] + '|' +\n",
    "    all_teachers_df['tDOB'].astype(str)\n",
    ")\n",
    "\n",
    "all_teachers_df['loose_key'] = (\n",
    "    all_teachers_df['tGiven'] + '|' +\n",
    "    all_teachers_df['tSurname']\n",
    ")\n",
    "\n",
    "# Step 3: Build lookup dictionaries\n",
    "strict_lookup = all_teachers_df.set_index('strict_key')['tPayroll'].to_dict()\n",
    "loose_lookup = all_teachers_df.set_index('loose_key')['tPayroll'].to_dict()\n",
    "\n",
    "# Step 4: Track initial nulls\n",
    "before_filled = df_staff_filtered['RMI SS#'].isna().sum()\n",
    "\n",
    "# Step 5: Try strict match first\n",
    "df_staff_filtered['RMI SS#_filled'] = df_staff_filtered.apply(\n",
    "    lambda row: strict_lookup.get(row['strict_key'], None) if pd.isna(row['RMI SS#']) else row['RMI SS#'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Fallback to loose match\n",
    "df_staff_filtered['RMI SS#_filled'] = df_staff_filtered.apply(\n",
    "    lambda row: loose_lookup.get(row['loose_key'], row['RMI SS#_filled']) if pd.isna(row['RMI SS#']) else row['RMI SS#_filled'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 7: Finalize column\n",
    "df_staff_filtered['RMI SS#'] = df_staff_filtered['RMI SS#_filled']\n",
    "df_staff_filtered.drop(columns=['RMI SS#_filled', 'strict_key', 'loose_key'], inplace=True)\n",
    "\n",
    "# Step 8: Summary\n",
    "after_filled = df_staff_filtered['RMI SS#'].isna().sum()\n",
    "filled_count = before_filled - after_filled\n",
    "\n",
    "print(f\"‚úÖ RMI SS# forward-fill complete with optional DOB matching.\")\n",
    "print(f\"üìå Initially missing: {before_filled}\")\n",
    "print(f\"‚úÖ Filled using strict or loose match: {filled_count}\")\n",
    "print(f\"‚ùå Still missing after both passes: {after_filled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6a412-58b2-4cf5-b8f3-26fdf86f3fa7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load valid qualification names and code-to-name mapping from lookup, excluding 'Academic Degree'\n",
    "qual_lookup = [\n",
    "    entry for entry in core_lookups['teacherQuals']\n",
    "    if entry.get('G') == 'Academic Degree'\n",
    "]\n",
    "\n",
    "valid_qual_names = {entry['N'].strip() for entry in qual_lookup}\n",
    "valid_qual_names_lower = {v.lower(): v for v in valid_qual_names}\n",
    "qual_code_to_name = {entry['C'].strip(): entry['N'].strip() for entry in qual_lookup}\n",
    "\n",
    "# Step 2: Extract unique raw qualifications\n",
    "raw_qualifications = df_staff_filtered['Highest Qualification'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping\n",
    "qualification_mapping = {\n",
    "    '2 yrs. Certificate': 'Certificate',\n",
    "    'AA': 'Associate of Arts',\n",
    "    'AA (Associates)': 'Associate of Arts',\n",
    "    'AA In Business Management': 'Associate of Arts',\n",
    "    'AA in Liberal Arts': 'Associate of Arts',\n",
    "    'AS': 'Associate of Science',\n",
    "    'AS (Associates)': 'Associate of Science',\n",
    "    'AS CCD': 'Associate of Science',\n",
    "    'AS Degree': 'Associate of Science',\n",
    "    'AS Degree in Elementary Education': 'Associate of Science',\n",
    "    'AS Degree in Liberal Arts': 'Associate of Science',\n",
    "    'AS Education': 'Associate of Science',\n",
    "    'AS In Liberal Arts': 'Associate of Science',\n",
    "    'AS degree': 'Associate of Science',\n",
    "    'AS degree Liber Arts': 'Associate of Science',\n",
    "    'AS in Business': 'Associate of Science',\n",
    "    'AS in Business/CCT': 'Associate of Science',\n",
    "    'AS in Education': 'Associate of Science',\n",
    "    'AS in Elem.Education': 'Associate of Science',\n",
    "    'AS in Elementary Eucation': 'Associate of Science',\n",
    "    'AS in Liberal Arts': 'Associate of Science',\n",
    "    'AS in Liberat Arts': 'Associate of Science',\n",
    "    'AS of Science': 'Associate of Science',\n",
    "    'AS( Associates)': 'Associate of Science',\n",
    "    'ASEE': 'Associate of Science',\n",
    "    'ASEE (Associates)': 'Associate of Science',\n",
    "    'ASsociate of Science': 'Associate of Science',\n",
    "    'As': 'Associate of Science',\n",
    "    'As Degree': 'Associate of Science',\n",
    "    'Associate': 'Associate of Science',\n",
    "    'Associate of Arts': 'Associate of Arts',\n",
    "    'Associate of Liberal Arts': 'Associate of Arts',\n",
    "    'Associate of Science': 'Associate of Science',\n",
    "    'Associates': 'Associate of Science',\n",
    "    'Associates/AS': 'Associate of Science',\n",
    "    'B.S': 'Bachelor of Science',\n",
    "    'BA': 'Bachelor of Arts',\n",
    "    'BA (Bachelors)': 'Bachelor of Arts',\n",
    "    'BA Elementary Education': 'Bachelor of Arts',\n",
    "    'BA In Public Health': 'Bachelor of Arts',\n",
    "    'BA Public Admin & Social Work': 'Bachelor of Arts',\n",
    "    'BA Sociology & Social work': 'Bachelor of Arts',\n",
    "    'BA degree': 'Bachelor of Arts',\n",
    "    'BA in Biblical': 'Bachelor of Arts',\n",
    "    'BA in Ed. Primary': 'Bachelor of Arts',\n",
    "    'BA in Education': 'Bachelor of Arts',\n",
    "    'BA in Theology': 'Bachelor of Arts',\n",
    "    'BA of Science in Education': 'Bachelor of Arts',\n",
    "    'BAEE': 'Bachelor of Arts',\n",
    "    'BAEE (Bachelors)': 'Bachelor of Arts',\n",
    "    'BED in Math & Physic': 'Bachelor of Education',\n",
    "    'BEd': 'Bachelor of Education',\n",
    "    'BS': 'Bachelor of Science',\n",
    "    'BS (Bachelors)': 'Bachelor of Science',\n",
    "    'BS in Edu': 'Bachelor of Science',\n",
    "    'BS-Culinary': 'Bachelor of Science',\n",
    "    'BSN': 'Bachelor of Science',\n",
    "    'Ba  in Education': 'Bachelor of Arts',\n",
    "    'Bachelor': 'Bachelor of Arts',\n",
    "    'Bachelor Degree with MA unit': 'Bachelor of Arts',\n",
    "    'Bachelor Science in nursing': 'Bachelor of Science',\n",
    "    'Bachelor in Secondary Education (English)': 'Bachelor of Education',\n",
    "    'Bachelor in Special Education': 'Bachelor of Education',\n",
    "    'Bachelor of Arts': 'Bachelor of Arts',\n",
    "    'Bachelor of Education': 'Bachelor of Education',\n",
    "    'Bachelor of Science': 'Bachelor of Science',\n",
    "    'Bachelor of Science in Business Admin': 'Bachelor of Science',\n",
    "    'Bachelor of Science in Education': 'Bachelor of Science',\n",
    "    'Bachelors': 'Bachelor of Arts',\n",
    "    'CECE': 'Certificate ECE',\n",
    "    'CECE (MOC)': 'Certificate ECE',\n",
    "    'CECE Certificate': 'Certificate ECE',\n",
    "    'CEDE': 'Certificate',\n",
    "    'CEDE Certificate': 'Certificate',\n",
    "    'CMI': 'Certificate',\n",
    "    'Cert ECE': 'Certificate ECE',\n",
    "    'Cert. ECE': 'Certificate ECE',\n",
    "    'Certificate': 'Certificate',\n",
    "    'Certificate ECE': 'Certificate ECE',\n",
    "    'Certificate in Carpentry': np.nan,\n",
    "    'Certificate in Organic & Agriculture': np.nan,\n",
    "    'Certificate of ECE': 'Certificate ECE',\n",
    "    'Certification of Completion in Teaching': 'Certification of Completion in Teaching',\n",
    "    'Classroom Teacher': np.nan,\n",
    "    'College': 'Certificate',\n",
    "    'DECE': 'Diploma ECE',\n",
    "    'DECEC': 'Diploma ECE',\n",
    "    'Degree': 'Certificate',\n",
    "    'Diploma': 'Diploma',\n",
    "    'Diploma ECE': 'Diploma ECE',\n",
    "    'Diploma ECECE': 'Diploma ECE',\n",
    "    'Diploma in Education': 'Diploma in Education',\n",
    "    'Diploma in Information Technology & Computer System ( Finishing up her BA in Computer System': 'Diploma',\n",
    "    'Diploma in Primary Education': 'Diploma in Education',\n",
    "    'Diploma of Community Social Services work APTC': 'Diploma',\n",
    "    'Diploma of Education': 'Diploma in Education',\n",
    "    'Diploma-IT & Networking': 'Diploma',\n",
    "    'ECE Certificate': 'Certificate ECE',\n",
    "    'ECEC': 'Certificate ECE',\n",
    "    'ECEC Certificate': 'Certificate ECE',\n",
    "    'Elem': 'Diploma in Education',\n",
    "    'Elementary Diploma': 'Diploma in Education',\n",
    "    'GED': 'Diploma in Education',\n",
    "    'GED (Certificate)': np.nan,\n",
    "    'GED (Diploma)': 'Diploma in Education',\n",
    "    'GED Diploma': 'Diploma in Education',\n",
    "    'H.S Diploma': 'High School',\n",
    "    'HPU Cert.': np.nan,\n",
    "    'HS': 'High School',\n",
    "    'HS (Diploma)': 'High School',\n",
    "    'HS Diploma': 'High School',\n",
    "    'HS/Job Corp': 'High School',\n",
    "    'High School': 'High School',\n",
    "    'High School Diploma': 'High School',\n",
    "    'High School diploma': 'High School',\n",
    "    'High Scool Diploma': 'High School',\n",
    "    'High school': 'High School',\n",
    "    'Highschool': 'High School',\n",
    "    'Hish School': 'High School',\n",
    "    'JSMI Bible Institute': 'Certificate',\n",
    "    'LA': 'Associate of Arts',\n",
    "    'LA (Associates)': 'Associate of Arts',\n",
    "    'Liberal Arts': 'Associate of Arts',\n",
    "    'M.ED': 'Masters of Education',\n",
    "    'M.ED (Masters)': 'Masters of Education',\n",
    "    'M.Ed': 'Masters of Education',\n",
    "    'MA': 'Masters of Arts',\n",
    "    'MA (Masters)': 'Masters of Arts',\n",
    "    'MA in Edu.': 'Masters of Education',\n",
    "    'MBA': 'Masters of Business Administration',\n",
    "    'MS': 'Masters of Science',\n",
    "    'Master': 'Masters Degree',\n",
    "    'Master Degree': 'Masters Degree',\n",
    "    'Master Education': 'Masters of Education',\n",
    "    'Master in Education': 'Masters of Education',\n",
    "    'Master of Education': 'Masters of Education',\n",
    "    'Master of Science': 'Masters of Science',\n",
    "    'Masters': 'Masters Degree',\n",
    "    'Masters In Curriculum Studies': 'Masters of Education',\n",
    "    'Masters in Education': 'Masters of Education',\n",
    "    'Masters of Arts': 'Masters of Arts',\n",
    "    'Masters of Education': 'Masters of Education',\n",
    "    'Masters of Science': 'Masters of Science',\n",
    "    'Not yet certified': np.nan,\n",
    "    'PhD Education': 'Doctor of Philosophy (PhD) in Education',\n",
    "    'Phd': 'Doctor of Philosophy (PhD)',\n",
    "    'Teacher Certificate': 'Certification of Education'\n",
    "}\n",
    "\n",
    "# Step 4: Fill in missing mappings using codes\n",
    "for val in raw_qualifications:\n",
    "    val_clean = val.strip()\n",
    "    current_mapped_value = qualification_mapping.get(val_clean)\n",
    "\n",
    "    if val_clean not in qualification_mapping or pd.isna(current_mapped_value):\n",
    "        if val_clean in qual_code_to_name:\n",
    "            qualification_mapping[val_clean] = qual_code_to_name[val_clean]\n",
    "\n",
    "# Step 5: Auto-mapping via exact, case-insensitive, or fuzzy match\n",
    "for val in raw_qualifications:\n",
    "    val_clean = val.strip()\n",
    "    val_lower = val_clean.lower()\n",
    "\n",
    "    if val_clean in qualification_mapping:\n",
    "        continue\n",
    "\n",
    "    if val_clean in valid_qual_names:\n",
    "        qualification_mapping[val_clean] = val_clean\n",
    "    elif val_lower in valid_qual_names_lower:\n",
    "        qualification_mapping[val_clean] = valid_qual_names_lower[val_lower]\n",
    "    else:\n",
    "        close = get_close_matches(val_clean, valid_qual_names, n=1, cutoff=0.8)\n",
    "        if close:\n",
    "            qualification_mapping[val_clean] = close[0]\n",
    "        else:\n",
    "            qualification_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 6: Apply mapping\n",
    "df_staff_filtered['CleanedQualification'] = (\n",
    "    df_staff_filtered['Highest Qualification']\n",
    "    .apply(lambda val: qualification_mapping.get(str(val).strip()) if pd.notna(val) else np.nan)\n",
    ")\n",
    "\n",
    "# Step 7: Summary\n",
    "print(\"üìã Final qualification mapping:\")\n",
    "for k, v in sorted(qualification_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "# Count only valid non-null qualifications\n",
    "valid_mask = df_staff_filtered['CleanedQualification'].isin(valid_qual_names)\n",
    "valid_count = valid_mask.sum()\n",
    "total_rows = df_staff_filtered.shape[0]\n",
    "null_count = df_staff_filtered['CleanedQualification'].isna().sum()\n",
    "invalid_count = total_rows - valid_count - null_count\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid qualifications: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped qualifications (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing qualifications (null): {null_count}\")\n",
    "\n",
    "# Step 8: Frequency breakdown\n",
    "qualification_counts = df_staff_filtered['CleanedQualification'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedQualification':\")\n",
    "print(qualification_counts)\n",
    "\n",
    "# Step 9: Sample invalid rows (not in valid list and not null and not marked as __UNMAPPED__)\n",
    "invalid_qual_df = df_staff_filtered[\n",
    "    ~df_staff_filtered['CleanedQualification'].isin(valid_qual_names) &\n",
    "    df_staff_filtered['CleanedQualification'].notna() &\n",
    "    (df_staff_filtered['CleanedQualification'] == \"__UNMAPPED__\")\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Highest Qualification' values:\")\n",
    "display(\n",
    "    invalid_qual_df[[\n",
    "        'School', 'First Name', 'Last Name', 'Highest Qualification', 'CleanedQualification'\n",
    "    ]].head(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada228f4-e11a-48e3-881f-493c633bc350",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load valid certification names and code-to-name mapping from lookup, filtered for 'RMI Certification'\n",
    "cert_lookup = [\n",
    "    entry for entry in core_lookups['teacherQuals']\n",
    "    if entry.get('G') == 'RMI Certification'\n",
    "]\n",
    "\n",
    "valid_cert_names = {entry['N'].strip() for entry in cert_lookup}\n",
    "valid_cert_names_lower = {v.lower(): v for v in valid_cert_names}\n",
    "cert_code_to_name = {entry['C'].strip(): entry['N'].strip() for entry in cert_lookup}\n",
    "\n",
    "# Step 2: Extract unique raw certification values\n",
    "raw_certifications = df_staff_filtered['Certification Level'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping (optional starter, you can extend this later as needed)\n",
    "certification_mapping = {\n",
    "    'No Certificate': np.nan,\n",
    "    'None': np.nan,\n",
    "    'Not Certified': np.nan,\n",
    "    'Not yet certified': np.nan,\n",
    "    'Peofessional III': 'Professional Certificate III',\n",
    "    'Prfofessional III': 'Professional Certificate III',\n",
    "    'Profesional I': 'Professional Certificate I',\n",
    "    'Professioan Certificate': 'Professional Certificate I',\n",
    "    'Professioanl Certificate': 'Professional Certificate I',\n",
    "    'Professional': 'Professional Certificate I',\n",
    "    'Professional 1': 'Professional Certificate I',\n",
    "    'Professional 2': 'Professional Certificate II',\n",
    "    'Professional 3': 'Professional Certificate III',\n",
    "    'Professional Certifate 1': 'Professional Certificate I',\n",
    "    'Professional Certificate': 'Professional Certificate I',\n",
    "    'Professional Certificate 1': 'Professional Certificate I',\n",
    "    'Professional Certificate I': 'Professional Certificate I',\n",
    "    'Professional Certificate II': 'Professional Certificate II',\n",
    "    'Professional I': 'Professional Certificate I',\n",
    "    'Professional II': 'Professional Certificate II',\n",
    "    'Professional III': 'Professional Certificate III',\n",
    "    'Proficient': 'Provisional Certificate',\n",
    "    'Provisional': 'Provisional Certificate',\n",
    "    'Provisional 1': 'Provisional Certificate',\n",
    "    'Provisional 2': 'Provisional Certificate',\n",
    "    'Provisional Certificate': 'Provisional Certificate',\n",
    "    'Provisional I': 'Provisional Certificate',\n",
    "    'Provisonal': 'Provisional Certificate',\n",
    "    'Provissional': 'Provisional Certificate',\n",
    "    'Standard Certificate': np.nan,\n",
    "    'UNknown': np.nan,\n",
    "    'Unknown': np.nan,\n",
    "    'None': np.nan,\n",
    "    'Not Certified': np.nan,\n",
    "    'Not yet certified': np.nan,\n",
    "    'No Certificate': np.nan\n",
    "}\n",
    "\n",
    "# Step 4: Fill in missing mappings by matching certification codes\n",
    "for val in raw_certifications:\n",
    "    val_clean = val.strip()\n",
    "    current_mapped_value = certification_mapping.get(val_clean)\n",
    "\n",
    "    if val_clean not in certification_mapping or pd.isna(current_mapped_value):\n",
    "        if val_clean in cert_code_to_name:\n",
    "            certification_mapping[val_clean] = cert_code_to_name[val_clean]\n",
    "\n",
    "# Step 5: Add auto-mapping for remaining unmatched\n",
    "for raw_value in raw_certifications:\n",
    "    val = raw_value.strip()\n",
    "    val_lower = val.lower()\n",
    "\n",
    "    if val in certification_mapping:\n",
    "        continue  # already mapped\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val in valid_cert_names:\n",
    "        certification_mapping[val] = val\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_cert_names_lower:\n",
    "        certification_mapping[val] = valid_cert_names_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val, valid_cert_names, n=1, cutoff=0.8)\n",
    "    if close:\n",
    "        certification_mapping[val] = close[0]\n",
    "    else:\n",
    "        certification_mapping[val] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 6: Tag originally missing values\n",
    "df_staff_filtered['__cert_missing'] = df_staff_filtered['Certification Level'].isna()\n",
    "\n",
    "# Step 7: Apply mapping\n",
    "def map_certification(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    stripped = str(val).strip()\n",
    "    return certification_mapping.get(stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedEdCertification'] = df_staff_filtered['Certification Level'].apply(map_certification)\n",
    "\n",
    "# Step 8: Summary\n",
    "print(\"üìã Final certification mapping:\")\n",
    "for k, v in sorted(certification_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "valid_mask = df_staff_filtered['CleanedEdCertification'].isin(valid_cert_names)\n",
    "valid_count = valid_mask.sum()\n",
    "null_count = df_staff_filtered['__cert_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedEdCertification'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid certifications: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped certifications (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing certifications (null): {null_count}\")\n",
    "\n",
    "# Step 9: Frequency breakdown\n",
    "cert_counts = df_staff_filtered['CleanedEdCertification'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedEdCertification':\")\n",
    "print(cert_counts)\n",
    "\n",
    "# Step 10: Sample invalid rows\n",
    "invalid_cert_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedEdCertification'] == \"__UNMAPPED__\"\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Certification Level' values:\")\n",
    "display(invalid_cert_df[['School', 'First Name', 'Last Name', 'Certification Level', 'CleanedEdCertification']].head(100))\n",
    "\n",
    "# Optional: clean up marker value if needed\n",
    "df_staff_filtered['CleanedEdCertification'] = df_staff_filtered['CleanedEdCertification'].replace(\"__UNMAPPED__\", np.nan)\n",
    "\n",
    "df_staff_filtered.drop(columns='__cert_missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38aafb1-1769-4c98-ad9a-786a1e415855",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Step 1: Load valid subject names from lookup\n",
    "subject_lookup = censusworkbook_lookups.get('subjects', [])\n",
    "valid_subject_names = {entry['N'].strip() for entry in subject_lookup if entry.get('N')}\n",
    "valid_subject_names_lower = {name.lower(): name for name in valid_subject_names}\n",
    "\n",
    "# Step 2: Extract raw field of study values\n",
    "raw_fields = df_staff_filtered['Field of Study'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Start with mapping directly from Field of Study\n",
    "field_mapping = {}\n",
    "\n",
    "for raw_val in raw_fields:\n",
    "    val = raw_val.strip()\n",
    "    val_lower = val.lower()\n",
    "\n",
    "    if val in valid_subject_names:\n",
    "        field_mapping[val] = val\n",
    "    elif val_lower in valid_subject_names_lower:\n",
    "        field_mapping[val] = valid_subject_names_lower[val_lower]\n",
    "    else:\n",
    "        close = get_close_matches(val, valid_subject_names, n=1, cutoff=0.8)\n",
    "        field_mapping[val] = close[0] if close else \"__UNMAPPED__\"\n",
    "\n",
    "# Step 4: Apply field_mapping\n",
    "df_staff_filtered['CleanedFieldofStudy'] = (\n",
    "    df_staff_filtered['Field of Study']\n",
    "    .apply(lambda val: field_mapping.get(str(val).strip()) if pd.notna(val) else np.nan)\n",
    ")\n",
    "\n",
    "# Step 5: Attempt to fill missing fields using 'Highest Qualification'\n",
    "def extract_possible_subject(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = str(text)\n",
    "    # Look for a subject-like phrase following 'in' or 'of'\n",
    "    matches = re.findall(r'in ([A-Za-z&\\.\\- ]+)|of ([A-Za-z&\\.\\- ]+)', text, flags=re.IGNORECASE)\n",
    "    possible_phrases = [m[0] or m[1] for m in matches]\n",
    "    for phrase in possible_phrases:\n",
    "        cleaned = phrase.strip().lower()\n",
    "        if cleaned in valid_subject_names_lower:\n",
    "            return valid_subject_names_lower[cleaned]\n",
    "        close = get_close_matches(cleaned, valid_subject_names, n=1, cutoff=0.8)\n",
    "        if close:\n",
    "            return close[0]\n",
    "    return None\n",
    "\n",
    "# Only fill in missing field of study values\n",
    "df_staff_filtered['CleanedFieldofStudy'] = df_staff_filtered.apply(\n",
    "    lambda row: extract_possible_subject(row['Highest Qualification']) if pd.isna(row['CleanedFieldofStudy']) else row['CleanedFieldofStudy'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 6: Replace unmapped with NaN\n",
    "df_staff_filtered['CleanedFieldofStudy'] = df_staff_filtered['CleanedFieldofStudy'].replace(\"__UNMAPPED__\", np.nan)\n",
    "\n",
    "# Step 7: Report\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedFieldofStudy':\")\n",
    "print(df_staff_filtered['CleanedFieldofStudy'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nüö´ Sample unmapped or questionable 'Field of Study':\")\n",
    "display(df_staff_filtered[\n",
    "    df_staff_filtered['CleanedFieldofStudy'].isna() &\n",
    "    df_staff_filtered['Field of Study'].notna()\n",
    "][['School', 'First Name', 'Last Name', 'Field of Study', 'Highest Qualification']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41de25-f9fc-4bd8-95d0-285025f53782",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define valid values\n",
    "valid_teaching_staff = {\"Teaching Staff\", \"Non Teaching Staff\"}\n",
    "valid_teaching_staff_lower = {v.lower(): v for v in valid_teaching_staff}\n",
    "\n",
    "# Step 2: Extract unique raw values\n",
    "raw_teaching_values = df_staff_filtered['Teaching Staff'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping starter (can be extended)\n",
    "teaching_staff_mapping = {\n",
    "    'Admin': 'Teaching Staff',  # ?\n",
    "    'Administration': 'Non Teaching Staff',  # ?\n",
    "    'Administrator': 'Teaching Staff',  # ?\n",
    "    'Head Teacher': 'Teaching Staff',\n",
    "    'N/A': np.nan,\n",
    "    'Non Teaching': 'Non Teaching Staff',\n",
    "    'Non Teaching Staff': 'Non Teaching Staff',\n",
    "    'Non teaching Staff': 'Non Teaching Staff',\n",
    "    'Non-Teaching': 'Non Teaching Staff',\n",
    "    'Non-Teaching Staff': 'Non Teaching Staff',\n",
    "    'NonTeaching': 'Non Teaching Staff',\n",
    "    'NonTeaching Staff': 'Non Teaching Staff',\n",
    "    'None': np.nan,\n",
    "    'Principal': 'Teaching Staff',  # ?\n",
    "    'SPED': 'Teaching Staff',  # ?\n",
    "    'SpEd Teacher': 'Teaching Staff',  # ?\n",
    "    \"T'eaching Staff\": 'Teaching Staff',\n",
    "    'Teacher': 'Teaching Staff',\n",
    "    'Teaching': 'Teaching Staff',\n",
    "    'Teaching Staff': 'Teaching Staff',\n",
    "    'Unknown': np.nan\n",
    "}\n",
    "\n",
    "# Step 4: Auto-mapping for remaining values\n",
    "for val in raw_teaching_values:\n",
    "    val_clean = val.strip()\n",
    "    val_lower = val_clean.lower()\n",
    "\n",
    "    if val_clean in teaching_staff_mapping:\n",
    "        continue\n",
    "\n",
    "    if val_clean in valid_teaching_staff:\n",
    "        teaching_staff_mapping[val_clean] = val_clean\n",
    "        continue\n",
    "\n",
    "    if val_lower in valid_teaching_staff_lower:\n",
    "        teaching_staff_mapping[val_clean] = valid_teaching_staff_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    close = get_close_matches(val_clean, valid_teaching_staff, n=1, cutoff=0.85)\n",
    "    if close:\n",
    "        teaching_staff_mapping[val_clean] = close[0]\n",
    "    else:\n",
    "        teaching_staff_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 5: Tag missing values\n",
    "df_staff_filtered['__teaching_missing'] = df_staff_filtered['Teaching Staff'].isna()\n",
    "\n",
    "# Step 6: Apply mapping\n",
    "def map_teaching(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    stripped = str(val).strip()\n",
    "    return teaching_staff_mapping.get(stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedTeachingStaff'] = df_staff_filtered['Teaching Staff'].apply(map_teaching)\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 7: Fill in 'Teaching Staff' if CleanedTeachingStaff is still NaN and Job Title contains words similar to \"teach\"\n",
    "def resembles_teaching(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    words = str(text).lower().split()\n",
    "    for word in words:\n",
    "        if get_close_matches(word, ['teach'], n=1, cutoff=0.7):  # lowered cutoff for lenient matching\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "mask_infer_teaching = (\n",
    "    df_staff_filtered['CleanedTeachingStaff'].isna() &\n",
    "    df_staff_filtered['Job Title'].apply(resembles_teaching)\n",
    ")\n",
    "\n",
    "df_staff_filtered.loc[mask_infer_teaching, 'CleanedTeachingStaff'] = \"Teaching Staff\"\n",
    "\n",
    "\n",
    "# Step 8: Summary\n",
    "print(\"üìã Final teaching staff mapping:\")\n",
    "for k, v in sorted(teaching_staff_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "valid_mask = df_staff_filtered['CleanedTeachingStaff'].isin(valid_teaching_staff)\n",
    "valid_count = valid_mask.sum()\n",
    "null_count = df_staff_filtered['__teaching_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedTeachingStaff'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid Teaching Staff values: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped values (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing values (null): {null_count}\")\n",
    "\n",
    "# Step 9: Frequency breakdown\n",
    "teaching_counts = df_staff_filtered['CleanedTeachingStaff'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedTeachingStaff':\")\n",
    "print(teaching_counts)\n",
    "\n",
    "# Step 10: Sample invalid and missing rows\n",
    "\n",
    "# Invalid (unmapped) values\n",
    "invalid_teaching_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedTeachingStaff'] == \"__UNMAPPED__\"\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Teaching Staff' values:\")\n",
    "display(invalid_teaching_df[['School', 'First Name', 'Last Name', 'Teaching Staff', 'CleanedTeachingStaff']].head(100))\n",
    "\n",
    "# Missing (NaN) values\n",
    "missing_teaching_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedTeachingStaff'].isna()\n",
    "]\n",
    "\n",
    "print(\"\\n‚¨ú Sample rows with missing 'Teaching Staff' values:\")\n",
    "display(missing_teaching_df[['School', 'First Name', 'Last Name', 'Teaching Staff', 'CleanedTeachingStaff']])\n",
    "\n",
    "# Step 11: Final cleanup\n",
    "df_staff_filtered['CleanedTeachingStaff'] = df_staff_filtered['CleanedTeachingStaff'].replace(\"__UNMAPPED__\", np.nan)\n",
    "df_staff_filtered.drop(columns='__teaching_missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec35fdf-ace8-4b8f-b900-09783fd27239",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load valid job titles from lookup\n",
    "role_lookup = censusworkbook_lookups.get('censusTeacherRoles', [])\n",
    "valid_roles = {entry['N'].strip() for entry in role_lookup if entry.get('N')}\n",
    "valid_roles_lower = {role.lower(): role for role in valid_roles}\n",
    "\n",
    "# Step 2: Extract unique raw job titles\n",
    "raw_job_titles = df_staff_filtered['Job Title'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping starter (can be extended later)\n",
    "job_title_mapping = {\n",
    "    '10th Counselor': 'Counselor',\n",
    "    '12th Counselor': 'Counselor',\n",
    "    '9th Counselor': 'Counselor',\n",
    "    'Academy Reistrar': 'Registrar',\n",
    "    'Accountant': 'Accountant',\n",
    "    'Accounting Clerk': 'Accountant',\n",
    "    'Acting Principal/Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Admin. Secretary': 'Secretary',\n",
    "    'Administrative Assisstant': '__UNMAPPED__',\n",
    "    'Adminstrator': '__UNMAPPED__',\n",
    "    'Adminstrator/Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Assistant Clerk': '__UNMAPPED__',\n",
    "    'Auto -teacher': 'Classroom Teacher I',\n",
    "    'Boat operator': '__UNMAPPED__',\n",
    "    'Budget Officer': '__UNMAPPED__',\n",
    "    'Bus Driver': 'Bus Driver',\n",
    "    'CLASSROOM TEACHER I': 'Classroom Teacher I',\n",
    "    'CLASSROOM TEACHER II': 'Classroom Teacher II',\n",
    "    'COOK': 'Cook',\n",
    "    'Cafeteria Worker': '__UNMAPPED__',\n",
    "    'Cafeteria Worker Supervisor': '__UNMAPPED__',\n",
    "    'Chaplain': '__UNMAPPED__',\n",
    "    'Chauffer/Driver': 'Bus Driver',\n",
    "    'Clasroom Teacher': 'Classroom Teacher I',\n",
    "    'Class Room Teacher/V-Principal': 'Classroom Teacher I',\n",
    "    'Classroom Teacer': 'Classroom Teacher I',\n",
    "    'Classroom Teache Aide': 'Classroom Teacher I',\n",
    "    'Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Classroom Teacher Aide': 'Classroom Teacher I',\n",
    "    'Classroom Teacher I': 'Classroom Teacher I',\n",
    "    'Classroom Teacher/Head Teacher': 'Classroom Teacher I',\n",
    "    'Classroom teacher': 'Classroom Teacher I',\n",
    "    'Classroon Teacher': 'Classroom Teacher I',\n",
    "    'Cook': 'Cook',\n",
    "    'Counselor': 'Counselor',\n",
    "    'Counselor / Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Counselor/Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Curriculum Specialist': '__UNMAPPED__',\n",
    "    'Custidoan': '__UNMAPPED__',\n",
    "    'Custodian': '__UNMAPPED__',\n",
    "    'Data Specialist': 'Data Specialist',\n",
    "    'Director': 'Director',\n",
    "    'Dorm Cook': 'Cook',\n",
    "    'Driver': 'Bud Driver',\n",
    "    'Elective Teacher': 'Teacher Aide',\n",
    "    'Female Counselor': 'Counselor',\n",
    "    'Financial Director': '__UNMAPPED__',\n",
    "    'Fiscal Officer': 'Fiscal Officer',\n",
    "    'Gardener Teacher': 'Classroom Teacher I',\n",
    "    'Girl''s Counselor': 'Counselor',\n",
    "    'H-Teacher/Teacher': 'Head Teacher I',\n",
    "    'Handy Man': '__UNMAPPED__',\n",
    "    'Head Teacher': 'Head Teacher I',\n",
    "    'Head Teacher/ Classroom Teacher': 'Head Teacher I',\n",
    "    'Head Teacher/(Principal)Classroom Teacher': 'Head Teacher I',\n",
    "    'Head Teacher/Clasroom Teacher': 'Head Teacher I',\n",
    "    'Head Teacher/Classroom Teacher': 'Head Teacher I',\n",
    "    'Head Teacher/Teacher': 'Head Teacher I',\n",
    "    'Heard Teacher/Classroom Teacher': 'Head Teacher I',\n",
    "    'House Baba': 'House Parent',\n",
    "    'House mama': 'House Parent',\n",
    "    'Housefather': 'House Parent',\n",
    "    'Housemother': 'House Parent',\n",
    "    'Houseparent': 'House Parent',\n",
    "    'IT': 'Information Technology Officer',\n",
    "    'IT/Computer/ Teacher': 'Information Technology Officer',\n",
    "    'Information Technology Officer': 'Information Technology Officer',\n",
    "    'It Trainee/STEM Specialist': 'Information Technology Officer',\n",
    "    'Janitor': '__UNMAPPED__',\n",
    "    'Janitor/ Support': '__UNMAPPED__',\n",
    "    'Janitoress support': '__UNMAPPED__',\n",
    "    'Janitress': '__UNMAPPED__',\n",
    "    'Kinder V-Principal': '__UNMAPPED__',\n",
    "    'Kitchen Staff': '__UNMAPPED__',\n",
    "    'Librarian': 'Librarian',\n",
    "    'MIHS Cook': 'Cook',\n",
    "    'MIHS Security': 'Security Guard',\n",
    "    'MLA Teacher': 'Classroom Teacher I',\n",
    "    'MLA Techer': 'Classroom Teacher I',\n",
    "    'Maintenace': 'Maintenance',\n",
    "    'Maintenance': 'Maintenance',\n",
    "    'Maintenance Support': 'Maintenance',\n",
    "    'Maintenance Support/Driver': 'Maintenance',\n",
    "    'Male Counselor': 'Counselor',\n",
    "    'Mantenance supervisor': '__UNMAPPED__',\n",
    "    'Mechanic': '__UNMAPPED__',\n",
    "    'N/A': np.nan,\n",
    "    'None': np.nan,\n",
    "    'Not Assigned': np.nan,\n",
    "    'Nurse': 'Nurse',\n",
    "    'Nurse / CT': 'Nurse',\n",
    "    'Office Assistant': '__UNMAPPED__',\n",
    "    'P.E. Teacher': 'Classroom Teacher I',\n",
    "    'Pre-9th (counselor)': 'Counselor',\n",
    "    'Principal': 'Principal (Primary)',\n",
    "    'Principal (Secondary)': 'Principal (Secondary)',\n",
    "    'Principal/Classroom Teacher': 'Head Teacher I',\n",
    "    'Principal/Clssroom Teacher': 'Head Teacher I',\n",
    "    'Principal/Teacher': 'Principal (Primary)',\n",
    "    'Prinicpal/Classroom Teacher': 'Head Teacher I',\n",
    "    'Recreations/Sports/PE': '__UNMAPPED__',\n",
    "    'Registrar': 'Registrar',\n",
    "    'SEP': '__UNMAPPED__',\n",
    "    'SPED': 'SEP Coordinator',\n",
    "    'SPED Coordinator': 'SEP Coordinator',\n",
    "    'SPED Teacher': 'Classroom Teacher I',\n",
    "    'SPED.Teacher': 'Classroom Teacher I',\n",
    "    'SPED/Classroom Teaher': 'Classroom Teacher I',\n",
    "    'SPED/RSA': 'SEP Coordinator',\n",
    "    'STEM Specialist': '__UNMAPPED__',\n",
    "    'School Admin -Vice Principal': 'Vice Principal (Primary)',\n",
    "    'School Admin- Principal': 'Principal (Primary)',\n",
    "    'School Counselor': 'Counselor',\n",
    "    'School Nurse': 'Nurse',\n",
    "    'School Secretary': 'Secretary',\n",
    "    'Secretary': 'Secretary',\n",
    "    'Secretary/ Classroom Teacher': 'Classroom Teacher I',\n",
    "    'Secretary/Collector': 'Secretary',\n",
    "    'Security': 'Security Guard',\n",
    "    'Security Guard': 'Security Guard',\n",
    "    'Securtity': 'Security Guard',\n",
    "    'SpEd Teacher': 'Classroom Teacher I',\n",
    "    'Special Education Teacher': 'Classroom Teacher I',\n",
    "    'Special Worker': 'Special Worker',\n",
    "    'Sped Teacher': 'Classroom Teacher I',\n",
    "    'Support Staff': '__UNMAPPED__',\n",
    "    'Teacher': 'Classroom Teacher I',\n",
    "    'Teacher Aid': 'Teacher Aide',\n",
    "    'Teacher Aide': 'Teacher Aide',\n",
    "    'Unassigned': np.nan,\n",
    "    'Unknown': np.nan,\n",
    "    'Unkown': np.nan,\n",
    "    'V-Principal': 'Vice Principal (Primary)',\n",
    "    'V.Principal': 'Vice Principal (Primary)',\n",
    "    'Vice Pri': 'Vice Principal (Primary)',\n",
    "    'Vice Principal': 'Vice Principal (Primary)',\n",
    "    'Vice Principal (Primary)': 'Vice Principal (Primary)',\n",
    "    'Vice Principal/Classroom Teacher': 'Head Teacher I',\n",
    "    'Vice principal': 'Vice Principal (Primary)',\n",
    "    'Vice-Principal': 'Vice Principal (Primary)',\n",
    "    'WASC Coordinator': 'SEP Coordinator',\n",
    "    'Watchman': '__UNMAPPED__',\n",
    "    'None': np.nan,\n",
    "    'N/A': np.nan,\n",
    "    'Not Assigned': np.nan,\n",
    "    'Unassigned': np.nan,\n",
    "    'Unkown': np.nan,\n",
    "    'Unknown': np.nan,\n",
    "}\n",
    "\n",
    "# Step 4: Fill in missing mappings by matching\n",
    "for val in raw_job_titles:\n",
    "    val_clean = val.strip()\n",
    "    current_mapped_value = job_title_mapping.get(val_clean)\n",
    "\n",
    "    if val_clean not in job_title_mapping or pd.isna(current_mapped_value):\n",
    "        # No code-to-name mapping needed for roles, so we skip that step\n",
    "\n",
    "        # Step 5: Auto-mapping\n",
    "        val_lower = val_clean.lower()\n",
    "\n",
    "        # Skip if already mapped\n",
    "        if val_clean in job_title_mapping:\n",
    "            continue\n",
    "\n",
    "        # 1. Exact match\n",
    "        if val_clean in valid_roles:\n",
    "            job_title_mapping[val_clean] = val_clean\n",
    "            continue\n",
    "\n",
    "        # 2. Case-insensitive match\n",
    "        if val_lower in valid_roles_lower:\n",
    "            job_title_mapping[val_clean] = valid_roles_lower[val_lower]\n",
    "            continue\n",
    "\n",
    "        # 3. Fuzzy match\n",
    "        close = get_close_matches(val_clean, valid_roles, n=1, cutoff=0.8)\n",
    "        if close:\n",
    "            job_title_mapping[val_clean] = close[0]\n",
    "        else:\n",
    "            job_title_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 6: Tag missing values\n",
    "df_staff_filtered['__job_missing'] = df_staff_filtered['Job Title'].isna()\n",
    "\n",
    "# Step 7: Apply mapping\n",
    "def map_job_title(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    stripped = str(val).strip()\n",
    "    return job_title_mapping.get(stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedJobTitle'] = df_staff_filtered['Job Title'].apply(map_job_title)\n",
    "\n",
    "# Step 8: Summary\n",
    "print(\"üìã Final job title mapping:\")\n",
    "for k, v in sorted(job_title_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "valid_mask = df_staff_filtered['CleanedJobTitle'].isin(valid_roles)\n",
    "valid_count = valid_mask.sum()\n",
    "null_count = df_staff_filtered['__job_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedJobTitle'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid job titles: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped job titles (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing job titles (null): {null_count}\")\n",
    "\n",
    "# Step 9: Frequency breakdown\n",
    "job_counts = df_staff_filtered['CleanedJobTitle'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedJobTitle':\")\n",
    "print(job_counts)\n",
    "\n",
    "# Step 10: Sample invalid rows (unmapped)\n",
    "invalid_job_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedJobTitle'] == \"__UNMAPPED__\"\n",
    "]\n",
    "\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Job Title' values:\")\n",
    "display(invalid_job_df[['School', 'First Name', 'Last Name', 'Job Title', 'CleanedJobTitle', 'CleanedTeachingStaff']])\n",
    "\n",
    "# Step 10.1: Sample rows with NaN\n",
    "nan_job_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedTeachingStaff'].isna()\n",
    "]\n",
    "\n",
    "print(\"\\n‚¨ú Sample rows with missing 'CleanedTeachingStaff' values (NaN):\")\n",
    "display(nan_job_df[['School', 'First Name', 'Last Name', 'Job Title', 'CleanedJobTitle', 'CleanedTeachingStaff']])\n",
    "\n",
    "# Optional: Replace marker value\n",
    "df_staff_filtered['CleanedJobTitle'] = df_staff_filtered['CleanedJobTitle'].replace(\"__UNMAPPED__\", np.nan)\n",
    "\n",
    "# Clean up temp column\n",
    "df_staff_filtered.drop(columns='__job_missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7e606-76b1-4a7a-ace5-97638a818c23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define valid values\n",
    "valid_status_values = {\"Active\", \"Inactive\"}\n",
    "valid_status_lower = {v.lower(): v for v in valid_status_values}\n",
    "\n",
    "# Step 2: Extract unique raw values\n",
    "raw_status_values = df_staff_filtered['Employment Status'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping starter (extend as needed)\n",
    "status_mapping = {\n",
    "    'Active': 'Active',\n",
    "    'Inactive': 'Inactive',\n",
    "    'Actve': 'Active',\n",
    "    'InActive': 'Inactive',\n",
    "    'Not Active': 'Inactive',\n",
    "    'Retired': 'Inactive',\n",
    "    'Still teaching': 'Active',\n",
    "    'Unknown': np.nan,\n",
    "    'None': np.nan,\n",
    "    'N/A': np.nan,\n",
    "    '': np.nan,\n",
    "}\n",
    "\n",
    "# Step 4: Auto-mapping for remaining values\n",
    "for val in raw_status_values:\n",
    "    val_clean = val.strip()\n",
    "    val_lower = val_clean.lower()\n",
    "\n",
    "    if val_clean in status_mapping:\n",
    "        continue\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val_clean in valid_status_values:\n",
    "        status_mapping[val_clean] = val_clean\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_status_lower:\n",
    "        status_mapping[val_clean] = valid_status_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val_clean, valid_status_values, n=1, cutoff=0.8)\n",
    "    if close:\n",
    "        status_mapping[val_clean] = close[0]\n",
    "    else:\n",
    "        status_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 5: Tag missing values\n",
    "df_staff_filtered['__status_missing'] = df_staff_filtered['Employment Status'].isna()\n",
    "\n",
    "# Step 6: Apply mapping\n",
    "def map_status(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    stripped = str(val).strip()\n",
    "    return status_mapping.get(stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedEmploymentStatus'] = df_staff_filtered['Employment Status'].apply(map_status)\n",
    "\n",
    "# Step 7: Summary\n",
    "print(\"üìã Final employment status mapping:\")\n",
    "for k, v in sorted(status_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "valid_mask = df_staff_filtered['CleanedEmploymentStatus'].isin(valid_status_values)\n",
    "valid_count = valid_mask.sum()\n",
    "null_count = df_staff_filtered['__status_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedEmploymentStatus'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid statuses: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped statuses (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing statuses (null): {null_count}\")\n",
    "\n",
    "# Step 8: Frequency breakdown\n",
    "status_counts = df_staff_filtered['CleanedEmploymentStatus'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedEmploymentStatus':\")\n",
    "print(status_counts)\n",
    "\n",
    "# Step 9: Sample invalid and missing rows\n",
    "invalid_status_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedEmploymentStatus'] == \"__UNMAPPED__\"\n",
    "]\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Employment Status' values:\")\n",
    "display(invalid_status_df[['School', 'First Name', 'Last Name', 'Employment Status', 'CleanedEmploymentStatus']].head(100))\n",
    "\n",
    "missing_status_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedEmploymentStatus'].isna()\n",
    "]\n",
    "print(\"\\n‚¨ú Sample rows with missing 'Employment Status' values:\")\n",
    "display(missing_status_df[['School', 'First Name', 'Last Name', 'Employment Status', 'CleanedEmploymentStatus']].head(100))\n",
    "\n",
    "# Step 10: Final cleanup\n",
    "df_staff_filtered['CleanedEmploymentStatus'] = df_staff_filtered['CleanedEmploymentStatus'].replace(\"__UNMAPPED__\", np.nan)\n",
    "df_staff_filtered.drop(columns='__status_missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a3b49-3bd4-4dbd-9ae9-e1b951cd9f97",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract valid organization names from lookup\n",
    "org_lookup = censusworkbook_lookups['organizations']\n",
    "valid_org_names = {entry['N'].strip() for entry in org_lookup}\n",
    "valid_org_names_lower = {v.lower(): v for v in valid_org_names}\n",
    "\n",
    "# Step 2: Extract unique raw values\n",
    "raw_org_values = df_staff_filtered['Organization'].dropna().astype(str).str.strip().unique()\n",
    "\n",
    "# Step 3: Manual mapping starter (extend as needed)\n",
    "org_mapping = {\n",
    "    '': np.nan,\n",
    "    'MOE': 'Ministry of Education',\n",
    "    'Ministry Education': 'Ministry of Education',\n",
    "    'N/A': np.nan,\n",
    "    'None': np.nan,\n",
    "    'PSS': 'PSS',\n",
    "    'Private Schools': 'Private School Teacher',\n",
    "    'Unknown': np.nan,\n",
    "}\n",
    "\n",
    "# Step 4: Auto-mapping for remaining values\n",
    "for val in raw_org_values:\n",
    "    val_clean = val.strip()\n",
    "    val_lower = val_clean.lower()\n",
    "\n",
    "    if val_clean in org_mapping:\n",
    "        continue\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val_clean in valid_org_names:\n",
    "        org_mapping[val_clean] = val_clean\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_org_names_lower:\n",
    "        org_mapping[val_clean] = valid_org_names_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val_clean, valid_org_names, n=1, cutoff=0.85)\n",
    "    if close:\n",
    "        org_mapping[val_clean] = close[0]\n",
    "    else:\n",
    "        org_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 5: Tag missing values\n",
    "df_staff_filtered['__org_missing'] = df_staff_filtered['Organization'].isna()\n",
    "\n",
    "# Step 6: Apply mapping\n",
    "def map_organization(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    stripped = str(val).strip()\n",
    "    return org_mapping.get(stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedOrganization'] = df_staff_filtered['Organization'].apply(map_organization)\n",
    "\n",
    "# Step 7: Infer organization from CleanedSchool via all_schools\n",
    "school_auth_map = {\n",
    "    school['schName'].strip(): school['schAuth'].strip() if school.get('schAuth') else None\n",
    "    for school in all_schools\n",
    "}\n",
    "\n",
    "# Function to infer from schAuth\n",
    "def infer_org_from_school(school_name):\n",
    "    auth = school_auth_map.get(str(school_name).strip())\n",
    "    if auth == \"PSS\":\n",
    "        return \"PSS\"\n",
    "    elif auth:\n",
    "        return \"Private School Teacher\"\n",
    "    return np.nan\n",
    "\n",
    "# Mask: only rows that still need inference\n",
    "mask_needs_inference = df_staff_filtered['CleanedOrganization'].isna() & df_staff_filtered['CleanedSchool'].notna()\n",
    "\n",
    "# Apply inference\n",
    "inferred_orgs = df_staff_filtered.loc[mask_needs_inference, 'CleanedSchool'].apply(infer_org_from_school)\n",
    "\n",
    "# Assign back to the DataFrame\n",
    "df_staff_filtered.loc[mask_needs_inference, 'CleanedOrganization'] = inferred_orgs\n",
    "\n",
    "# Count results\n",
    "pss_count = (inferred_orgs == \"PSS\").sum()\n",
    "private_count = (inferred_orgs == \"Private School Teacher\").sum()\n",
    "null_count = inferred_orgs.isna().sum()\n",
    "\n",
    "# Identify distinct schools that led to \"Private School Teacher\"\n",
    "private_school_names = df_staff_filtered.loc[\n",
    "    mask_needs_inference & (inferred_orgs == \"Private School Teacher\"),\n",
    "    'CleanedSchool'\n",
    "].dropna().unique()\n",
    "\n",
    "# Summary printout\n",
    "print(\"\\nüè´ Inferred 'Organization' from 'CleanedSchool':\")\n",
    "print(f\"   ‚úÖ PSS: {pss_count}\")\n",
    "print(f\"   üè´ Private School Teacher: {private_count}\")\n",
    "print(f\"       From schools: {', '.join(sorted(private_school_names))}\")\n",
    "print(f\"   ‚¨ú Still unknown (no match or no auth info): {null_count}\")\n",
    "\n",
    "\n",
    "# Step 8: Summary\n",
    "print(\"üìã Final organization mapping:\")\n",
    "for k, v in sorted(org_mapping.items()):\n",
    "    print(f\"'{k}': '{v}',\")\n",
    "\n",
    "valid_mask = df_staff_filtered['CleanedOrganization'].isin(valid_org_names)\n",
    "valid_count = valid_mask.sum()\n",
    "null_count = df_staff_filtered['__org_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedOrganization'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Auto-mapped valid organizations: {valid_count}\")\n",
    "print(f\"‚ùå Unmapped values (invalid): {invalid_count}\")\n",
    "print(f\"‚¨ú Missing values (null): {null_count}\")\n",
    "\n",
    "# Step 9: Frequency breakdown\n",
    "org_counts = df_staff_filtered['CleanedOrganization'].value_counts(dropna=False)\n",
    "print(\"\\nüìä Frequency breakdown of 'CleanedOrganization':\")\n",
    "print(org_counts)\n",
    "\n",
    "# Step 10: Sample invalid and missing rows\n",
    "invalid_org_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedOrganization'] == \"__UNMAPPED__\"\n",
    "]\n",
    "print(\"\\nüö´ Sample rows with invalid or unmapped 'Organization' values:\")\n",
    "display(invalid_org_df[['School', 'First Name', 'Last Name', 'Organization', 'CleanedOrganization']].head(100))\n",
    "\n",
    "missing_org_df = df_staff_filtered[\n",
    "    df_staff_filtered['CleanedOrganization'].isna()\n",
    "]\n",
    "print(\"\\n‚¨ú Sample rows with missing 'Organization' values:\")\n",
    "display(missing_org_df[['School', 'First Name', 'Last Name', 'Organization', 'CleanedOrganization']].head(100))\n",
    "\n",
    "# Step 11: Final cleanup\n",
    "df_staff_filtered['CleanedOrganization'] = df_staff_filtered['CleanedOrganization'].replace(\"__UNMAPPED__\", np.nan)\n",
    "df_staff_filtered.drop(columns='__org_missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d427e24-e00f-459c-a5e6-182b5d6b3673",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the columns to clean\n",
    "grade_columns = [\n",
    "    \"ECE\", \"Grade 1\", \"Grade 2\", \"Grade 3\", \"Grade 4\", \"Grade 5\",\n",
    "    \"Grade 6\", \"Grade 7\", \"Grade 8\", \"Pre-9\", \"Grade 9\", \"Grade 10\",\n",
    "    \"Grade 11\", \"Grade 12\", \"Admin\", \"Other\"\n",
    "]\n",
    "\n",
    "print(\"üìã Cleaning teaching assignment markers (interpreting any non-blank as 'x'):\")\n",
    "\n",
    "cleaned_column_names = []\n",
    "\n",
    "# Step 2: Clean each column\n",
    "for col in grade_columns:\n",
    "    cleaned_col = f\"Cleaned{col.replace(' ', '').replace('-', '')}\"\n",
    "    cleaned_column_names.append(cleaned_col)\n",
    "\n",
    "    # Step 3: Convert original values to lowercase and strip spaces\n",
    "    original = df_staff_filtered[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Step 4: Identify raw value breakdown before cleaning\n",
    "    counts = original.value_counts(dropna=False)\n",
    "    print(f\"\\nüìä Column: '{col}' ‚Äî raw value breakdown:\")\n",
    "    print(counts)\n",
    "\n",
    "    # Step 5: Treat any non-empty string (even weird ones like 'yes', 's', etc.) as 'x'\n",
    "    df_staff_filtered[cleaned_col] = original.apply(lambda x: \"x\" if x and x != \"nan\" else np.nan)\n",
    "\n",
    "    # Step 6: Summary stats\n",
    "    num_x = (df_staff_filtered[cleaned_col] == 'x').sum()\n",
    "    num_na = df_staff_filtered[cleaned_col].isna().sum()\n",
    "    total = len(df_staff_filtered)\n",
    "\n",
    "    print(f\"‚úÖ Cleaned column: '{cleaned_col}'\")\n",
    "    print(f\"   ‚úî Interpreted 'x' values: {num_x}\")\n",
    "    print(f\"   ‚¨ú Blank or empty (set to NaN): {num_na}\")\n",
    "    print(f\"   üì¶ Total: {total}\")\n",
    "\n",
    "print(\"\\n‚úÖ All cleaned columns created:\")\n",
    "print(cleaned_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a94d7a-79e7-46ff-9a48-9a8a029b9a23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Ensure CleanedOther column exists and is of type object\n",
    "if 'CleanedOther' not in df_staff_filtered.columns:\n",
    "    df_staff_filtered['CleanedOther'] = np.nan\n",
    "\n",
    "df_staff_filtered['CleanedOther'] = df_staff_filtered['CleanedOther'].astype(object)\n",
    "\n",
    "# Apply inference safely\n",
    "mask_infer_other = (\n",
    "    (df_staff_filtered['CleanedTeachingStaff'] == \"Non Teaching Staff\") &\n",
    "    (df_staff_filtered['CleanedOther'].isna())\n",
    ")\n",
    "\n",
    "df_staff_filtered.loc[mask_infer_other, 'CleanedOther'] = \"x\"\n",
    "\n",
    "# Summary of changes\n",
    "inferred_other_count = mask_infer_other.sum()\n",
    "total_other_x = (df_staff_filtered['CleanedOther'] == \"x\").sum()\n",
    "total_rows = len(df_staff_filtered)\n",
    "\n",
    "print(\"\\nüß© Inference for 'CleanedOther':\")\n",
    "print(f\"   üÜï Inferred from 'Non Teaching Staff': {inferred_other_count}\")\n",
    "print(f\"   ‚úÖ Total 'x' values in CleanedOther: {total_other_x}\")\n",
    "print(f\"   üì¶ Total records: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b865df7-48ee-4491-a06d-7d1bb07b7380",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Step 1: Extract valid values from the lookup\n",
    "teacher_type_entries = censusworkbook_lookups['teacherRegStatus']\n",
    "valid_teacher_types = [entry['N'] for entry in teacher_type_entries]\n",
    "valid_teacher_types_set = set(valid_teacher_types)\n",
    "valid_teacher_types_lower = {v.lower(): v for v in valid_teacher_types}\n",
    "\n",
    "# Step 2: Build mapping from raw values\n",
    "teacher_type_mapping = {\n",
    "    'Regular': 'Regular',\n",
    "    'Special Ed': 'Special Ed',\n",
    "    'Teacher-Aide': 'Assistant'\n",
    "}\n",
    "\n",
    "raw_values = df_staff_filtered['Teacher-Type'].dropna().unique()\n",
    "\n",
    "for val in raw_values:\n",
    "    val_clean = str(val).strip()\n",
    "    val_lower = val_clean.lower()\n",
    "\n",
    "    # 1. Exact match\n",
    "    if val_clean in valid_teacher_types_set:\n",
    "        teacher_type_mapping[val_clean] = val_clean\n",
    "        continue\n",
    "\n",
    "    # 2. Case-insensitive match\n",
    "    if val_lower in valid_teacher_types_lower:\n",
    "        teacher_type_mapping[val_clean] = valid_teacher_types_lower[val_lower]\n",
    "        continue\n",
    "\n",
    "    # 3. Fuzzy match\n",
    "    close = get_close_matches(val_clean, valid_teacher_types, n=1, cutoff=0.85)\n",
    "    if close:\n",
    "        teacher_type_mapping[val_clean] = close[0]\n",
    "    else:\n",
    "        teacher_type_mapping[val_clean] = \"__UNMAPPED__\"\n",
    "\n",
    "# Step 3: Tag missing values\n",
    "df_staff_filtered['__teacher_type_missing'] = df_staff_filtered['Teacher-Type'].isna()\n",
    "\n",
    "# Step 4: Apply mapping\n",
    "def map_teacher_type(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    val_stripped = str(val).strip()\n",
    "    return teacher_type_mapping.get(val_stripped, \"__UNMAPPED__\")\n",
    "\n",
    "df_staff_filtered['CleanedTeacherType'] = df_staff_filtered['Teacher-Type'].apply(map_teacher_type)\n",
    "\n",
    "# Step 5: Summary\n",
    "valid_count = df_staff_filtered['CleanedTeacherType'].isin(valid_teacher_types).sum()\n",
    "null_count = df_staff_filtered['__teacher_type_missing'].sum()\n",
    "invalid_count = (df_staff_filtered['CleanedTeacherType'] == \"__UNMAPPED__\").sum()\n",
    "\n",
    "print(\"\\nüßë‚Äçüè´ Cleaned 'Teacher-Type' ‚Üí 'CleanedTeacherType':\")\n",
    "print(f\"   ‚úÖ Valid entries: {valid_count}\")\n",
    "print(f\"   ‚ùå Unmapped entries: {invalid_count}\")\n",
    "print(f\"   ‚¨ú Missing entries: {null_count}\")\n",
    "\n",
    "# Step 6: Mapping summary\n",
    "print(\"\\nüìã Mapping used:\")\n",
    "for k, v in sorted(teacher_type_mapping.items()):\n",
    "    print(f\"'{k}': '{v}'\")\n",
    "\n",
    "# Step: Infer 'CleanedTeacherType' as 'Regular' if missing but marked as Teaching Staff\n",
    "mask_infer_regular = (\n",
    "    df_staff_filtered['CleanedTeacherType'].isna() &\n",
    "    (df_staff_filtered['CleanedTeachingStaff'] == \"Teaching Staff\")\n",
    ")\n",
    "\n",
    "df_staff_filtered.loc[mask_infer_regular, 'CleanedTeacherType'] = \"Regular\"\n",
    "\n",
    "# Summary\n",
    "print(f\"‚úÖ Inferred 'Regular' for missing 'CleanedTeacherType' where staff is marked as 'Teaching Staff': {mask_infer_regular.sum()} rows updated.\")\n",
    "\n",
    "# Step 7: Frequency breakdown\n",
    "print(\"\\nüìä Frequency breakdown:\")\n",
    "print(df_staff_filtered['CleanedTeacherType'].value_counts(dropna=False))\n",
    "\n",
    "# Step 8: Invalid samples\n",
    "invalid_df = df_staff_filtered[df_staff_filtered['CleanedTeacherType'] == \"__UNMAPPED__\"]\n",
    "print(\"\\nüö´ Sample rows with unmapped 'Teacher-Type' values:\")\n",
    "display(invalid_df[['School', 'First Name', 'Last Name', 'Teacher-Type', 'CleanedTeacherType']].head(100))\n",
    "\n",
    "# Step 9: Missing samples\n",
    "missing_df = df_staff_filtered[df_staff_filtered['CleanedTeacherType'].isna()]\n",
    "print(\"\\n‚¨ú Sample rows with missing 'Teacher-Type' values:\")\n",
    "display(missing_df[['School', 'First Name', 'Last Name', 'Teacher-Type', 'CleanedTeacherType']].head(100))\n",
    "\n",
    "# Step 10: Final cleanup\n",
    "df_staff_filtered['CleanedTeacherType'] = df_staff_filtered['CleanedTeacherType'].replace(\"__UNMAPPED__\", np.nan)\n",
    "df_staff_filtered.drop(columns=\"__teacher_type_missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54437e9f-93b1-4af2-b6e7-2774307b45a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 1: Copy and preprocess the source column\n",
    "raw_dates = df_staff_filtered['Date of Hire'].astype(str).str.strip()\n",
    "\n",
    "# Step 2: Try parsing full dates directly\n",
    "cleaned_dates = pd.to_datetime(raw_dates, errors='coerce', dayfirst=False)\n",
    "\n",
    "# Step 3: Handle year-only values like \"2020\"\n",
    "year_only_mask = cleaned_dates.isna() & raw_dates.str.fullmatch(r\"\\d{4}\")\n",
    "parsed_years = pd.to_datetime(raw_dates[year_only_mask] + \"-10-01\", errors='coerce')\n",
    "cleaned_dates[year_only_mask] = parsed_years\n",
    "\n",
    "# Step 4: Fill in missing values from df_teacher_recent_survey_data using name + schNo match\n",
    "missing_mask = cleaned_dates.isna()\n",
    "\n",
    "df_recent = df_teacher_recent_survey_data[['tGiven', 'tSurname', 'tPayroll', 'tDatePSAppointed', 'schNo']].copy()\n",
    "\n",
    "df_recent['tKey'] = (\n",
    "    df_recent['tGiven'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_recent['tSurname'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_recent['schNo'].astype(str).str.strip().str.upper()\n",
    ")\n",
    "\n",
    "df_staff_filtered['nameKey'] = (\n",
    "    df_staff_filtered['First Name'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_staff_filtered['Last Name'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_staff_filtered['schNo'].astype(str).str.strip().str.upper()\n",
    ")\n",
    "\n",
    "# Check for duplicates in tKey\n",
    "duplicate_keys = df_recent[df_recent.duplicated(subset='tKey', keep=False)]\n",
    "\n",
    "print(\"\\nüö® Duplicate tKey entries found in df_teacher_recent_survey_data:\")\n",
    "print(duplicate_keys.sort_values('tKey').to_string(index=False))\n",
    "\n",
    "# Prioritize rows with non-null tDatePSAppointed and tPayroll\n",
    "df_recent = df_recent.sort_values(\n",
    "    by=['tDatePSAppointed', 'tPayroll'], \n",
    "    ascending=[False, False]\n",
    ")\n",
    "\n",
    "# Then drop duplicates, keeping the first (which now has more complete data)\n",
    "df_recent = df_recent.drop_duplicates(subset='tKey', keep='first')\n",
    "\n",
    "matched_dates = df_staff_filtered.loc[missing_mask, 'nameKey'].map(\n",
    "    df_recent.set_index('tKey')['tDatePSAppointed']\n",
    ")\n",
    "\n",
    "matched_dates_parsed = pd.to_datetime(matched_dates, errors='coerce')\n",
    "cleaned_dates[missing_mask] = matched_dates_parsed\n",
    "\n",
    "# Assign final cleaned date column\n",
    "df_staff_filtered['CleanedDateofHire'] = cleaned_dates\n",
    "\n",
    "# Summary\n",
    "num_parsed = cleaned_dates.notna().sum()\n",
    "num_total = len(cleaned_dates)\n",
    "num_filled_from_recent = matched_dates_parsed.notna().sum()\n",
    "\n",
    "print(\"üìÖ Date of Hire Cleaning Summary:\")\n",
    "print(f\"‚úÖ Total parsed or inferred: {num_parsed} / {num_total}\")\n",
    "print(f\"   üîÑ Filled from recent survey data (name + school match): {num_filled_from_recent}\")\n",
    "print(f\"   ‚¨ú Still missing: {(df_staff_filtered['CleanedDateofHire'].isna()).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543889e-d62d-4b19-9441-a7ea17996b3f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 1: Create unique keys for matching\n",
    "df_recent = df_teacher_recent_survey_data[['tGiven', 'tSurname', 'tPayroll', 'tchSalary', 'schNo']].copy()\n",
    "\n",
    "df_recent['tKey'] = (\n",
    "    df_recent['tGiven'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_recent['tSurname'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_recent['schNo'].astype(str).str.strip().str.upper()\n",
    ")\n",
    "\n",
    "df_staff_filtered['nameKey'] = (\n",
    "    df_staff_filtered['First Name'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_staff_filtered['Last Name'].astype(str).str.strip().str.lower() + \"|\" +\n",
    "    df_staff_filtered['schNo'].astype(str).str.strip().str.upper()\n",
    ")\n",
    "\n",
    "# Step 2: Check for duplicates in tKey\n",
    "duplicate_keys_salary = df_recent[df_recent.duplicated(subset='tKey', keep=False)]\n",
    "print(\"\\nüö® Duplicate tKey entries found in df_teacher_recent_survey_data (Salary context):\")\n",
    "print(duplicate_keys_salary.sort_values('tKey').to_string(index=False))\n",
    "\n",
    "# Step 3: Prioritize rows with non-null tchSalary and drop duplicates\n",
    "df_recent = df_recent.sort_values(\n",
    "    by=['tchSalary', 'tPayroll'], \n",
    "    ascending=[False, False]\n",
    ")\n",
    "\n",
    "df_recent = df_recent.drop_duplicates(subset='tKey', keep='first')\n",
    "\n",
    "# Step 4: Perform the mapping for missing salary values\n",
    "salary_matched = df_staff_filtered['nameKey'].map(\n",
    "    df_recent.set_index('tKey')['tchSalary']\n",
    ")\n",
    "\n",
    "# Step 5: Assign final cleaned salary\n",
    "df_staff_filtered['CleanedAnnualSalary'] = pd.to_numeric(salary_matched, errors='coerce')\n",
    "\n",
    "# Step 6: Summary\n",
    "total_filled = df_staff_filtered['CleanedAnnualSalary'].notna().sum()\n",
    "total_rows = len(df_staff_filtered)\n",
    "\n",
    "print(\"üí∞ Annual Salary Cleaning Summary:\")\n",
    "print(f\"‚úÖ Total filled from matched teacher survey data: {total_filled} / {total_rows}\")\n",
    "print(f\"‚¨ú Still missing: {(df_staff_filtered['CleanedAnnualSalary'].isna()).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216f344-d5cd-42a3-b2d5-c6bd67c20f7c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "new_workbook_path = os.path.join(output_directory, empty_census_workbook_filename)\n",
    "clean_workbook_path = os.path.join(output_directory, clean_census_workbook_filename)\n",
    "\n",
    "# üßπ Delete and copy clean workbook template (controlled by flag)\n",
    "if delete_census_workbook_filename:\n",
    "    if os.path.exists(new_workbook_path):\n",
    "        try:\n",
    "            os.remove(new_workbook_path)\n",
    "            print(f\"üóëÔ∏è Removed previous workbook: {new_workbook_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to delete existing workbook: {new_workbook_path}\\n{e}\")\n",
    "\n",
    "    try:\n",
    "        shutil.copyfile(clean_workbook_path, new_workbook_path)\n",
    "        print(f\"üìÑ Copied clean template to: {new_workbook_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to copy clean workbook template.\\n{e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping deletion and copy of census workbook (using existing workbook).\")\n",
    "\n",
    "\n",
    "# üìå Mapping cleaned columns ‚Üí Excel columns\n",
    "column_mapping = {\n",
    "    'CleanedYear': 'SchoolYear',\n",
    "    #'dName': 'Atoll / Island',\n",
    "    'CleanedSchool': 'School Name',\n",
    "    #'schNo': 'PSS School ID',\n",
    "    'CleanedOrganization': 'Organization',\n",
    "    #'Office': 'Office',\n",
    "    'First Name': 'First Name',\n",
    "    'Middle Name': 'Middle Name',\n",
    "    'Last Name': 'Last Name',\n",
    "    #'Full Name': 'Full Name',\n",
    "    'Gender': 'Gender',\n",
    "    'CleanedDateofBirth': 'Date of Birth',\n",
    "    #'Age': 'Age',\n",
    "    'CleanedCitizenship': 'Citizenship',\n",
    "    'CleanedEthnicity': 'Ethnicity',\n",
    "    'RMI SS#': 'RMI SSN',\n",
    "    #'Other SS#': 'Other SSN',\n",
    "    'CleanedQualification': 'Highest Qualification',\n",
    "    'CleanedFieldofStudy': 'Field of Study',\n",
    "    'Year of Completion': 'Year of Completion',\n",
    "    'CleanedEdCertification': 'Highest Ed Certification',\n",
    "    #'Year Of Completion.1': 'Year Of Completion2',\n",
    "    'CleanedEmploymentStatus': 'Employment Status',\n",
    "    'Reason': 'Reason',\n",
    "    'CleanedJobTitle': 'Job Title',\n",
    "    'CleanedOrganization': 'Organization',\n",
    "    'CleanedTeachingStaff': 'Staff Type',\n",
    "    'CleanedTeacherType': 'Teacher Type',\n",
    "    'CleanedDateofHire': 'Date of Hire',\n",
    "    'Date of Exit': 'Date Of Exit',\n",
    "    'CleanedAnnualSalary': 'Annual Salary',\n",
    "    'Funding Source': 'Funding Source',\n",
    "    'CleanedECE': 'ECE',\n",
    "    'CleanedGrade1': 'Grade 1',\n",
    "    'CleanedGrade2': 'Grade 2',\n",
    "    'CleanedGrade3': 'Grade 3',\n",
    "    'CleanedGrade4': 'Grade 4',\n",
    "    'CleanedGrade5': 'Grade 5',\n",
    "    'CleanedGrade6': 'Grade 6',\n",
    "    'CleanedGrade7': 'Grade 7',\n",
    "    'CleanedGrade8': 'Grade 8',\n",
    "    #'CleanedPre9': 'Grade 9',\n",
    "    'CleanedGrade9': 'Grade 9',\n",
    "    'CleanedGrade10': 'Grade 10',\n",
    "    'CleanedGrade11': 'Grade 11',\n",
    "    'CleanedGrade12': 'Grade 12',\n",
    "    'CleanedAdmin': 'Admin',\n",
    "    'CleanedOther': 'Other',\n",
    "    #'Total Days Absence': 'Total Days Absence',\n",
    "    #'Maths': 'Maths',\n",
    "    #'Science': 'Science',\n",
    "    #'Language': 'Language',\n",
    "    #'Competency': 'Competency',\n",
    "    #'Teach Mathematics': 'Teach Mathematics',\n",
    "    #'Teach Language Arts': 'Teach Language Arts',\n",
    "    #'Teach Social Studies': 'Teach Social Studies',\n",
    "    #'Teach Sciences': 'Teach Sciences'\n",
    "}\n",
    "\n",
    "try:\n",
    "    app = xw.App(visible=False)\n",
    "    wb = app.books.open(new_workbook_path)\n",
    "    \n",
    "    # Get the correct worksheet\n",
    "    sheet_name = 'SchoolStaff'\n",
    "    ws = wb.sheets[sheet_name]\n",
    "    \n",
    "    # ‚úÖ Unprotect the correct sheet\n",
    "    ws.api.Unprotect()  # Add password if needed\n",
    "\n",
    "    # Prepare data\n",
    "    df_to_insert = df_staff_filtered[list(column_mapping.keys())].copy()\n",
    "    df_to_insert.rename(columns=column_mapping, inplace=True)\n",
    "    df_to_insert = df_to_insert.astype(object).where(pd.notna(df_to_insert), None)\n",
    "\n",
    "    print('df_to_insert: ')\n",
    "    display(df_to_insert.head(3))    \n",
    "    print(df_to_insert.columns)\n",
    "    \n",
    "    # Locate header row\n",
    "    header_row = 3\n",
    "    excel_headers = ws.range((header_row, 1)).expand('right').value\n",
    "    header_indices = {\n",
    "        header: idx + 1 for idx, header in enumerate(excel_headers) if header in df_to_insert.columns\n",
    "    }\n",
    "\n",
    "    # Write to Excel\n",
    "    start_row = header_row + 1\n",
    "    num_rows = len(df_to_insert)\n",
    "    invalid_columns = []\n",
    "\n",
    "    for col_name, col_idx in header_indices.items():\n",
    "        try:\n",
    "            col_values = df_to_insert[col_name].tolist()\n",
    "            ws.range((start_row, col_idx), (start_row + num_rows - 1, col_idx)).value = [[v] for v in col_values]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in column: {col_name} (Excel column {col_idx})\")\n",
    "            print(e)\n",
    "            invalid_columns.append(col_name)\n",
    "\n",
    "    if len(invalid_columns) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Columns that failed to write: {invalid_columns}\")\n",
    "\n",
    "    wb.save()\n",
    "finally:\n",
    "    wb.close()\n",
    "    app.quit()\n",
    "\n",
    "print(\"‚úÖ Staff data successfully injected into SchoolStaff sheet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af642c77-3b82-40ef-998c-0971acfe5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staff_filtered.columns"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
